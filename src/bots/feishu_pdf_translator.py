"""
PDFç¿»è¯‘é£žä¹¦æœåŠ¡

å¤„ç†é£žä¹¦æ¶ˆæ¯ä¸­çš„PDFé“¾æŽ¥ï¼Œè‡ªåŠ¨ä¸‹è½½å¹¶ç¿»è¯‘ï¼Œç„¶åŽå‘å›žé£žä¹¦ã€‚
"""

import logging
import os
import re
import time
import uuid
from pathlib import Path
from typing import Optional
import urllib.request

import requests

from src.paper_translator.paper_translator.processor import PaperTranslator
from src.paper_translator.paper_translator.config import config as pdf_config

logger = logging.getLogger(__name__)


class FeishuPDFTranslationService:
    """é£žä¹¦PDFç¿»è¯‘æœåŠ¡"""

    # æ”¯æŒçš„PDF URLæ¨¡å¼
    PDF_URL_PATTERNS = [
        r'https?://[^\s]+\.pdf',
        r'https?://[^\s]+/paper/[^\s]+\.pdf',
        r'https?://[^\s]+/pdf/[^\s]+\.pdf',
        r'https?://arxiv\.org/pdf/[^\s]+\.pdf',
    ]

    def __init__(self, config: dict):
        self.config = config
        self.enabled = config.get('enabled', False)

        # åˆ›å»ºè¾“å…¥è¾“å‡ºç›®å½•
        self.input_dir = Path(config.get('input_dir', 'data/papers/input'))
        self.output_dir = Path(config.get('output_dir', 'data/papers/output'))
        self.input_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        if self.enabled:
            self._init_translator()

        # é£žä¹¦é…ç½®
        self.feishu_config = config.get('feishu', {})

        # ç¼“å­˜ç¿»è¯‘æ–‡æœ¬çš„å¯¹è±¡ï¼ˆç”¨äºŽç½‘é¡µç¿»è¯‘ï¼‰
        self._text_translator = None

    def _get_text_translator(self):
        """èŽ·å–æ–‡æœ¬ç¿»è¯‘å™¨ï¼ˆç”¨äºŽç½‘é¡µç¿»è¯‘ï¼‰"""
        if self._text_translator is None:
            from src.paper_translator.paper_translator.translation_engine import TranslationEngine
            # ä½¿ç”¨ä¸Ž PDF ç¿»è¯‘ç›¸åŒçš„é…ç½®
            provider = 'minimax'
            self._text_translator = TranslationEngine(
                provider=provider,
                api_key=self.config.get('minimax', {}).get('api_key'),
                base_url=self.config.get('minimax', {}).get('base_url'),
                model=self.config.get('minimax', {}).get('model', 'MiniMax-Text-01')
            )
        return self._text_translator

        logger.info(f"FeishuPDFTranslationService initialized: enabled={self.enabled}")

    def _init_translator(self):
        """åˆå§‹åŒ–ç¿»è¯‘å™¨"""
        # é…ç½®è®ºæ–‡ç¿»è¯‘ç³»ç»Ÿ
        pdf_config._config['deepseek_api_key'] = self.config.get('deepseek', {}).get('api_key', '')
        pdf_config._config['deepseek_base_url'] = self.config.get('deepseek', {}).get('base_url', 'https://api.deepseek.com')
        pdf_config._config['deepseek_model'] = self.config.get('deepseek', {}).get('model', 'deepseek-chat')
        pdf_config._config['siliconflow_api_key'] = self.config.get('siliconflow', {}).get('api_key', '')
        pdf_config._config['output_dir'] = str(self.output_dir)

        self.translator = PaperTranslator()
        logger.info("PDF Translator initialized")

    def is_pdf_url(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«PDFé“¾æŽ¥"""
        for pattern in self.PDF_URL_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False

    def extract_pdf_url(self, text: str) -> Optional[str]:
        """ä»Žæ–‡æœ¬ä¸­æå–PDFé“¾æŽ¥"""
        for pattern in self.PDF_URL_PATTERNS:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        return None

    def process_pdf_link(
        self,
        pdf_url: str,
        user_id: Optional[str] = None,
        chat_id: Optional[str] = None,
        feishu_client: Optional[object] = None
    ) -> dict:
        """
        å¤„ç†PDFé“¾æŽ¥

        Args:
            pdf_url: PDFæ–‡ä»¶URL
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºŽé€šçŸ¥ï¼‰
            chat_id: ç¾¤èŠID
            feishu_client: é£žä¹¦å®¢æˆ·ç«¯ï¼ˆç”¨äºŽå‘é€æ¶ˆæ¯ï¼‰

        Returns:
            å¤„ç†ç»“æžœå­—å…¸
        """
        if not self.enabled:
            return {
                'success': False,
                'message': 'PDFç¿»è¯‘æœåŠ¡æœªå¯ç”¨'
            }

        logger.info(f"å¼€å§‹å¤„ç†: {pdf_url}")
        start_time = time.time()

        try:
            # 0. å¤„ç†è¾“å…¥ï¼Œåˆ¤æ–­æ˜¯ PDF è¿˜æ˜¯ç½‘é¡µ
            cleaned_url = pdf_url.strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯ arXiv IDï¼ˆå¦‚ 2501.12345ï¼‰
            import re
            if re.match(r'^\d{4}\.\d{4,5}$', cleaned_url):
                # è½¬æ¢ä¸º arXiv PDF URL
                cleaned_url = f"https://arxiv.org/pdf/{cleaned_url}.pdf"
                logger.info(f"å·²å°† arXiv ID è½¬æ¢ä¸º PDF URL: {cleaned_url}")
            elif cleaned_url.startswith('arxiv:'):
                arxiv_id = cleaned_url[6:].strip()
                cleaned_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                logger.info(f"å·²å°† arXiv ID è½¬æ¢ä¸º PDF URL: {cleaned_url}")
            elif cleaned_url.startswith('http://') or cleaned_url.startswith('https://'):
                # æ˜¯ HTTP URLï¼Œæ£€æŸ¥æ˜¯ PDF è¿˜æ˜¯ç½‘é¡µ
                if '.pdf' in cleaned_url.lower() or '/pdf/' in cleaned_url.lower():
                    # PDF URL
                    pass
                else:
                    # ç½‘é¡µ URLï¼Œä½¿ç”¨ç½‘é¡µç¿»è¯‘
                    logger.info(f"æ£€æµ‹ä¸ºç½‘é¡µ URLï¼Œä½¿ç”¨ç½‘é¡µç¿»è¯‘: {cleaned_url}")
                    return self._translate_webpage(cleaned_url)

            # 1. ä¸‹è½½PDF
            pdf_path = self._download_pdf(cleaned_url)
            if not pdf_path:
                # ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä½œä¸ºç½‘é¡µå¤„ç†
                logger.info("PDF ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä½œä¸ºç½‘é¡µå¤„ç†")
                return self._translate_webpage(cleaned_url)

            logger.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_path}")

            # 2. ç¿»è¯‘PDF
            result = self.translator.translate(str(pdf_path))

            # 3. å‘é€ç»“æžœåˆ°é£žä¹¦
            if feishu_client:
                self._send_to_feishu(
                    feishu_client,
                    chat_id,
                    user_id,
                    result
                )

            processing_time = time.time() - start_time

            return {
                'success': True,
                'message': 'ç¿»è¯‘å®Œæˆ',
                'output_path': result.output_path,
                'processing_time': processing_time,
                'stats': {
                    'pages': result.total_pages,
                    'terms': len(result.all_terms),
                    'formulas': len(result.all_formulas),
                    'figures': len(result.all_figures)
                }
            }

        except Exception as e:
            logger.error(f"PDFç¿»è¯‘å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'ç¿»è¯‘å¤±è´¥: {str(e)}'
            }

    def _download_pdf(self, url: str) -> Optional[Path]:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        try:
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            filename = f"{uuid.uuid4().hex[:8]}.pdf"
            output_path = self.input_dir / filename

            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            request = urllib.request.Request(url, headers=headers)

            with urllib.request.urlopen(request, timeout=60) as response:
                with open(output_path, 'wb') as f:
                    f.write(response.read())

            return output_path

        except Exception as e:
            logger.error(f"ä¸‹è½½PDFå¤±è´¥: {e}")
            return None

    def _fetch_webpage(self, url: str) -> Optional[dict]:
        """èŽ·å–ç½‘é¡µå†…å®¹"""
        try:
            from bs4 import BeautifulSoup

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            }

            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = response.apparent_encoding or 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                script.decompose()

            # èŽ·å–æ ‡é¢˜
            title = soup.title.string if soup.title else ''
            if not title:
                title = soup.find('h1')
                title = title.get_text(strip=True) if title else url

            # èŽ·å–ä¸»è¦å†…å®¹
            content = soup.find('article') or soup.find('main') or soup.find('div', class_=lambda x: x and 'content' in x.lower())
            if content:
                text = content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)

            # é™åˆ¶æ–‡æœ¬é•¿åº¦
            max_length = 50000
            if len(text) > max_length:
                text = text[:max_length] + '\n\n... [å†…å®¹å·²æˆªæ–­]'

            logger.info(f"ç½‘é¡µèŽ·å–æˆåŠŸ: {url}, æ ‡é¢˜: {title}, å†…å®¹é•¿åº¦: {len(text)}")
            return {
                'title': title,
                'content': text,
                'url': url
            }

        except Exception as e:
            logger.error(f"èŽ·å–ç½‘é¡µå¤±è´¥: {e}")
            return None

    def _translate_webpage(self, url: str) -> dict:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹"""
        logger.info(f"å¼€å§‹ç¿»è¯‘ç½‘é¡µ: {url}")

        # èŽ·å–ç½‘é¡µå†…å®¹
        webpage = self._fetch_webpage(url)
        if not webpage:
            return {
                'success': False,
                'message': 'èŽ·å–ç½‘é¡µå†…å®¹å¤±è´¥'
            }

        # ç¿»è¯‘æ ‡é¢˜
        translator = self._get_text_translator()
        try:
            title_translated = translator.translate_text(
                f"è¯·ç¿»è¯‘ä»¥ä¸‹æ ‡é¢˜ä¸ºä¸­æ–‡ï¼š{webpage['title']}",
                style="casual"
            )
            # æå–ç¿»è¯‘åŽçš„æ ‡é¢˜
            if 'ç¿»è¯‘ï¼š' in title_translated:
                title_translated = title_translated.split('ç¿»è¯‘ï¼š')[-1].strip()
        except Exception as e:
            logger.warning(f"æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
            title_translated = webpage['title']

        # ç¿»è¯‘å†…å®¹ï¼ˆåˆ†æ®µå¤„ç†ï¼‰
        content = webpage['content']
        max_chunk = 8000
        chunks = [content[i:i+max_chunk] for i in range(0, len(content), max_chunk)]

        translated_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ç¿»è¯‘ç½‘é¡µå†…å®¹ chunk {i+1}/{len(chunks)}")
            try:
                translated = translator.translate_text(
                    f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒåŽŸæ–‡æ ¼å¼ï¼š\n\n{chunk}",
                    style="casual"
                )
                translated_chunks.append(translated)
            except Exception as e:
                logger.warning(f"å†…å®¹ç¿»è¯‘å¤±è´¥ chunk {i+1}: {e}")
                translated_chunks.append(chunk)

        translated_content = '\n\n'.join(translated_chunks)

        return {
            'success': True,
            'message': 'ç½‘é¡µç¿»è¯‘å®Œæˆ',
            'title': webpage['title'],
            'title_translated': title_translated,
            'content': translated_content,
            'url': url
        }

    def _send_to_feishu(
        self,
        feishu_client,
        chat_id: Optional[str],
        user_id: Optional[str],
        result
    ):
        """å‘é€ç¿»è¯‘ç»“æžœåˆ°é£žä¹¦"""
        try:
            # æž„å»ºæ¶ˆæ¯å†…å®¹
            message = f"âœ… è®ºæ–‡ç¿»è¯‘å®Œæˆï¼\n\n"
            message += f"ðŸ“„ æ ‡é¢˜: {result.title}\n"
            message += f"ðŸ“Š é¡µæ•°: {result.total_pages}\n"
            message += f"ðŸ“– æœ¯è¯­æ•°: {len(result.all_terms)}\n"
            message += f"ðŸ”¢ å…¬å¼æ•°: {len(result.all_formulas)}\n"
            message += f"ðŸ–¼ï¸ å›¾è¡¨æ•°: {len(result.all_figures)}\n"
            message += f"â±ï¸ å¤„ç†æ—¶é—´: {result.processing_time:.1f}ç§’\n\n"

            if result.output_path:
                # ä¸Šä¼ æ–‡ä»¶åˆ°é£žä¹¦
                file_url = self._upload_file_to_feishu(feishu_client, result.output_path)
                if file_url:
                    message += f"ðŸ“¥ ä¸‹è½½ç¿»è¯‘åŽçš„PDF: {file_url}"

            # å‘é€åˆ°ç¾¤èŠ
            if chat_id and hasattr(feishu_client, 'send_message'):
                feishu_client.send_message(chat_id, message)
            elif user_id and hasattr(feishu_client, 'send_dm'):
                feishu_client.send_dm(user_id, message)

        except Exception as e:
            logger.error(f"å‘é€é£žä¹¦æ¶ˆæ¯å¤±è´¥: {e}")

    def _upload_file_to_feishu(self, feishu_client, file_path: str) -> Optional[str]:
        """ä¸Šä¼ æ–‡ä»¶åˆ°é£žä¹¦å¹¶è¿”å›žä¸‹è½½é“¾æŽ¥"""
        try:
            # é£žä¹¦æ–‡ä»¶ä¸Šä¼ API
            upload_url = "https://open.feishu.cn/open-apis/drive/v1/files/upload_all"

            with open(file_path, 'rb') as f:
                files = {'file': (Path(file_path).name, f, 'application/pdf')}
                data = {
                    'parent_node': 'root',
                    'file_type': 'pdf'
                }
                headers = {
                    'Authorization': f'Bearer {feishu_client.access_token}'
                } if hasattr(feishu_client, 'access_token') else {}

                response = requests.post(
                    upload_url,
                    files=files,
                    data=data,
                    headers=headers,
                    timeout=60
                )

            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    return result.get('data', {}).get('download_url')

        except Exception as e:
            logger.error(f"ä¸Šä¼ æ–‡ä»¶åˆ°é£žä¹¦å¤±è´¥: {e}")

        return None
