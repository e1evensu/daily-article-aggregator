"""
è¯é¢˜è¿½è¸ªå™¨
Topic Tracker

è¿½è¸ªå’Œåˆ†æçƒ­é—¨è¯é¢˜ï¼Œæ£€æµ‹è¯é¢˜çªå¢ã€‚
Tracks and analyzes hot topics, detects topic spikes.

Requirements:
- 12.1: å…³é”®è¯æå–
- 12.2: é¢‘ç‡èšåˆ
- 12.3: è¶‹åŠ¿è®¡ç®—
- 12.4: çªå¢æ£€æµ‹
- 12.5: è¯é¢˜æ’å
"""

import logging
import re
from collections import Counter
from datetime import datetime, timedelta
from typing import Optional

from src.stats.models import TopicFrequency
from src.stats.store import StatsStore

logger = logging.getLogger(__name__)


# åœç”¨è¯åˆ—è¡¨ï¼ˆä¸­è‹±æ–‡å¸¸è§åœç”¨è¯ï¼‰
STOP_WORDS = {
    # ä¸­æ–‡åœç”¨è¯
    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
    'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å¯ä»¥', 'èƒ½', 'å—', 'å‘¢',
    'å•Š', 'å“¦', 'å—¯', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»–ä»¬',
    # è‹±æ–‡åœç”¨è¯
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
    'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
    'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
    'into', 'through', 'during', 'before', 'after', 'above', 'below',
    'between', 'under', 'again', 'further', 'then', 'once', 'here',
    'there', 'when', 'where', 'why', 'how', 'all', 'each', 'few', 'more',
    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
    'same', 'so', 'than', 'too', 'very', 'just', 'and', 'but', 'if', 'or',
    'because', 'until', 'while', 'about', 'against', 'this', 'that',
    'these', 'those', 'what', 'which', 'who', 'whom', 'i', 'me', 'my',
    'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',
    'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',
    'theirs', 'themselves',
}


class TopicTracker:
    """
    è¯é¢˜è¿½è¸ªå™¨
    Topic Tracker
    
    ä»é—®ç­”æŸ¥è¯¢ä¸­æå–å…³é”®è¯ï¼Œè¿½è¸ªè¯é¢˜é¢‘ç‡å’Œè¶‹åŠ¿ã€‚
    Extracts keywords from QA queries, tracks topic frequency and trends.
    
    Attributes:
        store: ç»Ÿè®¡æ•°æ®å­˜å‚¨
               Statistics data store
        min_keyword_length: æœ€å°å…³é”®è¯é•¿åº¦
                            Minimum keyword length
        spike_threshold: çªå¢æ£€æµ‹é˜ˆå€¼ï¼ˆç›¸å¯¹äºå¹³å‡å€¼çš„å€æ•°ï¼‰
                         Spike detection threshold (multiple of average)
    
    Examples:
        >>> tracker = TopicTracker()
        >>> topics = tracker.get_trending_topics(days=7)
        >>> for topic in topics:
        ...     print(f"{topic.topic}: {topic.frequency} ({topic.trend})")
    
    Requirements: 12.1, 12.2, 12.3, 12.4, 12.5
    """
    
    def __init__(
        self,
        store: StatsStore | None = None,
        db_path: str = 'data/stats.db',
        min_keyword_length: int = 2,
        spike_threshold: float = 3.0
    ):
        """
        åˆå§‹åŒ–è¯é¢˜è¿½è¸ªå™¨
        Initialize Topic Tracker
        
        Args:
            store: ç»Ÿè®¡æ•°æ®å­˜å‚¨ï¼ˆå¯é€‰ï¼Œç”¨äºä¾èµ–æ³¨å…¥ï¼‰
                   Statistics data store (optional, for dependency injection)
            db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå½“ store ä¸º None æ—¶ä½¿ç”¨ï¼‰
                     Database path (used when store is None)
            min_keyword_length: æœ€å°å…³é”®è¯é•¿åº¦
                                Minimum keyword length
            spike_threshold: çªå¢æ£€æµ‹é˜ˆå€¼
                             Spike detection threshold
        """
        self.store = store or StatsStore(db_path)
        self.min_keyword_length = min_keyword_length
        self.spike_threshold = spike_threshold
        
        # å°è¯•åŠ è½½ jiebaï¼ˆå¯é€‰ï¼‰
        self._jieba = None
        try:
            import jieba
            self._jieba = jieba
            logger.info("jieba loaded for Chinese word segmentation")
        except ImportError:
            logger.info("jieba not available, using simple tokenization")
    
    def extract_keywords(self, text: str) -> list[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        Extract keywords from text
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
                  Input text
        
        Returns:
            å…³é”®è¯åˆ—è¡¨
            List of keywords
        
        Examples:
            >>> tracker.extract_keywords("ä»€ä¹ˆæ˜¯ RAG æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Ÿ")
            ['RAG', 'æ£€ç´¢', 'å¢å¼º', 'ç”Ÿæˆ']
        
        Requirements: 12.1
        """
        if not text:
            return []
        
        keywords = []
        
        # ä½¿ç”¨ jieba åˆ†è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._jieba:
            words = self._jieba.cut(text)
            for word in words:
                word = word.strip()
                if self._is_valid_keyword(word):
                    keywords.append(word)
        else:
            # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
            keywords = self._simple_tokenize(text)
        
        return keywords
    
    def _simple_tokenize(self, text: str) -> list[str]:
        """
        ç®€å•åˆ†è¯
        Simple tokenization
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
                  Input text
        
        Returns:
            è¯åˆ—è¡¨
            List of words
        """
        # åˆ†å‰²ä¸­è‹±æ–‡
        # åŒ¹é…ä¸­æ–‡è¯ï¼ˆè¿ç»­ä¸­æ–‡å­—ç¬¦ï¼‰å’Œè‹±æ–‡è¯ï¼ˆè¿ç»­å­—æ¯æ•°å­—ï¼‰
        pattern = r'[\u4e00-\u9fff]+|[a-zA-Z][a-zA-Z0-9]*'
        matches = re.findall(pattern, text)
        
        keywords = []
        for word in matches:
            word = word.strip()
            if self._is_valid_keyword(word):
                keywords.append(word)
        
        return keywords
    
    def _is_valid_keyword(self, word: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå…³é”®è¯
        Check if it's a valid keyword
        
        Args:
            word: è¯
                  Word
        
        Returns:
            True å¦‚æœæ˜¯æœ‰æ•ˆå…³é”®è¯
            True if it's a valid keyword
        """
        if not word:
            return False
        
        # é•¿åº¦æ£€æŸ¥
        if len(word) < self.min_keyword_length:
            return False
        
        # åœç”¨è¯æ£€æŸ¥
        if word.lower() in STOP_WORDS:
            return False
        
        # çº¯æ•°å­—æ£€æŸ¥
        if word.isdigit():
            return False
        
        return True
    
    def get_topic_frequencies(
        self,
        days: int = 7,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
        limit: int = 100
    ) -> list[TopicFrequency]:
        """
        è·å–è¯é¢˜é¢‘ç‡
        Get topic frequencies
        
        ä»é—®ç­”æŸ¥è¯¢ä¸­æå–å…³é”®è¯å¹¶ç»Ÿè®¡é¢‘ç‡ã€‚
        Extracts keywords from QA queries and counts frequencies.
        
        Args:
            days: ç»Ÿè®¡å¤©æ•°ï¼ˆå½“ start_time/end_time æœªæŒ‡å®šæ—¶ä½¿ç”¨ï¼‰
                  Number of days (used when start_time/end_time not specified)
            start_time: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼‰
                        Start time (optional)
            end_time: ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼‰
                      End time (optional)
            limit: è¿”å›æ•°é‡é™åˆ¶
                   Return count limit
        
        Returns:
            è¯é¢˜é¢‘ç‡åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡é™åºï¼‰
            List of topic frequencies (sorted by frequency descending)
        
        Examples:
            >>> topics = tracker.get_topic_frequencies(days=7, limit=20)
            >>> for topic in topics:
            ...     print(f"{topic.topic}: {topic.frequency}")
        
        Requirements: 12.2
        """
        # è®¡ç®—æ—¶é—´èŒƒå›´
        if not end_time:
            end_time = datetime.now()
        if not start_time:
            start_time = end_time - timedelta(days=days)
        
        # è·å–é—®ç­”äº‹ä»¶
        events = self.store.get_qa_events(
            start_time=start_time,
            end_time=end_time,
            limit=100000
        )
        
        # æå–å…³é”®è¯å¹¶ç»Ÿè®¡
        keyword_counter: Counter = Counter()
        keyword_first_seen: dict[str, datetime] = {}
        keyword_last_seen: dict[str, datetime] = {}
        
        for event in events:
            keywords = self.extract_keywords(event.query)
            for keyword in keywords:
                keyword_counter[keyword] += 1
                
                # è®°å½•é¦–æ¬¡å’Œæœ€åå‡ºç°æ—¶é—´
                if keyword not in keyword_first_seen:
                    keyword_first_seen[keyword] = event.timestamp
                keyword_last_seen[keyword] = event.timestamp
        
        # æ„å»ºè¯é¢˜é¢‘ç‡åˆ—è¡¨
        topics = []
        for keyword, frequency in keyword_counter.most_common(limit):
            topics.append(TopicFrequency(
                topic=keyword,
                frequency=frequency,
                first_seen=keyword_first_seen.get(keyword),
                last_seen=keyword_last_seen.get(keyword)
            ))
        
        return topics
    
    def get_trending_topics(
        self,
        days: int = 7,
        compare_days: int = 7,
        limit: int = 20
    ) -> list[TopicFrequency]:
        """
        è·å–è¶‹åŠ¿è¯é¢˜
        Get trending topics
        
        æ¯”è¾ƒå½“å‰å‘¨æœŸå’Œä¸Šä¸€å‘¨æœŸçš„è¯é¢˜é¢‘ç‡ï¼Œè®¡ç®—è¶‹åŠ¿ã€‚
        Compares topic frequencies between current and previous periods,
        calculates trends.
        
        Args:
            days: å½“å‰å‘¨æœŸå¤©æ•°
                  Current period days
            compare_days: å¯¹æ¯”å‘¨æœŸå¤©æ•°
                          Comparison period days
            limit: è¿”å›æ•°é‡é™åˆ¶
                   Return count limit
        
        Returns:
            è¶‹åŠ¿è¯é¢˜åˆ—è¡¨ï¼ˆåŒ…å«è¶‹åŠ¿ä¿¡æ¯ï¼‰
            List of trending topics (with trend information)
        
        Examples:
            >>> topics = tracker.get_trending_topics(days=7)
            >>> for topic in topics:
            ...     print(f"{topic.topic}: {topic.trend} ({topic.change_rate:+.1%})")
        
        Requirements: 12.3
        """
        now = datetime.now()
        
        # å½“å‰å‘¨æœŸ
        current_start = now - timedelta(days=days)
        current_topics = self.get_topic_frequencies(
            start_time=current_start,
            end_time=now,
            limit=1000
        )
        current_freq = {t.topic: t.frequency for t in current_topics}
        
        # ä¸Šä¸€å‘¨æœŸ
        prev_end = current_start
        prev_start = prev_end - timedelta(days=compare_days)
        prev_topics = self.get_topic_frequencies(
            start_time=prev_start,
            end_time=prev_end,
            limit=1000
        )
        prev_freq = {t.topic: t.frequency for t in prev_topics}
        
        # è®¡ç®—è¶‹åŠ¿
        trending = []
        for topic in current_topics[:limit]:
            current = current_freq.get(topic.topic, 0)
            previous = prev_freq.get(topic.topic, 0)
            
            # è®¡ç®—å˜åŒ–ç‡
            if previous > 0:
                change_rate = (current - previous) / previous
            elif current > 0:
                change_rate = 1.0  # æ–°è¯é¢˜
            else:
                change_rate = 0.0
            
            # åˆ¤æ–­è¶‹åŠ¿
            if change_rate > 0.2:
                trend = 'rising'
            elif change_rate < -0.2:
                trend = 'falling'
            else:
                trend = 'stable'
            
            # æ£€æµ‹çªå¢
            avg_freq = (current + previous) / 2 if previous > 0 else current / 2
            is_spike = current > avg_freq * self.spike_threshold if avg_freq > 0 else False
            
            trending.append(TopicFrequency(
                topic=topic.topic,
                frequency=topic.frequency,
                trend=trend,
                change_rate=change_rate,
                first_seen=topic.first_seen,
                last_seen=topic.last_seen,
                is_spike=is_spike
            ))
        
        return trending
    
    def detect_spikes(
        self,
        days: int = 1,
        baseline_days: int = 7,
        limit: int = 10
    ) -> list[TopicFrequency]:
        """
        æ£€æµ‹è¯é¢˜çªå¢
        Detect topic spikes
        
        æ£€æµ‹ç›¸å¯¹äºåŸºçº¿å‘¨æœŸé¢‘ç‡çªå¢çš„è¯é¢˜ã€‚
        Detects topics with frequency spikes relative to baseline period.
        
        Args:
            days: æ£€æµ‹å‘¨æœŸå¤©æ•°
                  Detection period days
            baseline_days: åŸºçº¿å‘¨æœŸå¤©æ•°
                           Baseline period days
            limit: è¿”å›æ•°é‡é™åˆ¶
                   Return count limit
        
        Returns:
            çªå¢è¯é¢˜åˆ—è¡¨
            List of spike topics
        
        Examples:
            >>> spikes = tracker.detect_spikes(days=1, baseline_days=7)
            >>> for topic in spikes:
            ...     print(f"ğŸ”¥ {topic.topic}: {topic.frequency} ({topic.change_rate:+.1%})")
        
        Requirements: 12.4
        """
        trending = self.get_trending_topics(
            days=days,
            compare_days=baseline_days,
            limit=100
        )
        
        # è¿‡æ»¤å‡ºçªå¢è¯é¢˜
        spikes = [t for t in trending if t.is_spike]
        
        # æŒ‰å˜åŒ–ç‡é™åºæ’åº
        spikes.sort(key=lambda x: x.change_rate, reverse=True)
        
        return spikes[:limit]
    
    def get_topic_history(
        self,
        topic: str,
        days: int = 30
    ) -> list[tuple[str, int]]:
        """
        è·å–è¯é¢˜å†å²é¢‘ç‡
        Get topic history frequency
        
        Args:
            topic: è¯é¢˜/å…³é”®è¯
                   Topic/keyword
            days: ç»Ÿè®¡å¤©æ•°
                  Number of days
        
        Returns:
            (æ—¥æœŸ, é¢‘ç‡) å…ƒç»„åˆ—è¡¨
            List of (date, frequency) tuples
        
        Requirements: 13.2
        """
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        # è·å–é—®ç­”äº‹ä»¶
        events = self.store.get_qa_events(
            start_time=start_time,
            end_time=end_time,
            limit=100000
        )
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡è¯é¢˜å‡ºç°æ¬¡æ•°
        daily_counts: dict[str, int] = {}
        for event in events:
            keywords = self.extract_keywords(event.query)
            if topic in keywords:
                date_str = event.timestamp.strftime('%Y-%m-%d')
                daily_counts[date_str] = daily_counts.get(date_str, 0) + 1
        
        # å¡«å……ç¼ºå¤±æ—¥æœŸ
        result = []
        current = start_time
        while current <= end_time:
            date_str = current.strftime('%Y-%m-%d')
            result.append((date_str, daily_counts.get(date_str, 0)))
            current += timedelta(days=1)
        
        return result
