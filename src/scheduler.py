"""
è°ƒåº¦å™¨æ¨¡å—
Scheduler Module

å®ç°å®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼Œæ‰§è¡Œå®Œæ•´çš„çˆ¬å–-åˆ†æ-æ¨é€æµç¨‹ã€‚
Implements scheduled task execution for the complete fetch-analyze-push workflow.

éœ€æ±‚ 7.1: æ”¯æŒå®šæ—¶æ‰§è¡Œï¼ˆæ¯å¤©æŒ‡å®šæ—¶é—´ï¼‰
éœ€æ±‚ 7.2: æ”¯æŒæ‰‹åŠ¨è§¦å‘æ‰§è¡Œ
éœ€æ±‚ 7.3: æ‰§è¡Œå®Œæ•´çš„çˆ¬å–-åˆ†æ-æ¨é€æµç¨‹
éœ€æ±‚ 7.4: è®°å½•æ‰§è¡Œæ—¥å¿—
éœ€æ±‚ 7.5: æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
"""

import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

import schedule

from src.config import load_config, get_config_value
from src.fetchers.arxiv_fetcher import ArxivFetcher
from src.fetchers.rss_fetcher import RSSFetcher
from src.processors.content_processor import ContentProcessor
from src.analyzers.ai_analyzer import AIAnalyzer
from src.repository import ArticleRepository
from src.bots.feishu_bot import FeishuBot, FeishuAppBot

# Import new fetchers and components
try:
    from src.fetchers.dblp_fetcher import DBLPFetcher
    from src.fetchers.nvd_fetcher import NVDFetcher
    from src.fetchers.kev_fetcher import KEVFetcher
    from src.fetchers.huggingface_fetcher import HuggingFaceFetcher
    from src.fetchers.web_blog_fetcher import HunyuanFetcher, AnthropicRedFetcher, AtumBlogFetcher
    from src.fetchers.github_fetcher import GitHubFetcher
    from src.fetchers.pwc_fetcher import PWCFetcher
    from src.fetchers.blog_fetcher import BlogFetcher
    from src.filters.vulnerability_filter import VulnerabilityFilter
    from src.scoring.priority_scorer import PriorityScorer
    from src.pushers.tiered_pusher import TieredPusher
    from src.utils.deduplication import deduplicate_by_url
    ADVANCED_FEATURES = True
except ImportError:
    ADVANCED_FEATURES = False

# Import Feishu Bitable
try:
    from src.bots.feishu_bitable import FeishuBitable
    BITABLE_AVAILABLE = True
except ImportError:
    BITABLE_AVAILABLE = False

# Import Checkpoint Manager
try:
    from src.utils.checkpoint import CheckpointManager
    CHECKPOINT_AVAILABLE = True
except ImportError:
    CHECKPOINT_AVAILABLE = False

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class FeishuAppBotWrapper:
    """
    FeishuAppBot åŒ…è£…ç±»ï¼Œç”¨äºå…¼å®¹ FeishuBot æ¥å£

    å°† FeishuAppBot çš„ API è°ƒç”¨é€‚é…ä¸º FeishuBot æ¥å£ï¼Œ
    ä»¥ä¾¿åœ¨ç°æœ‰ä»£ç ä¸­ä½¿ç”¨åº”ç”¨ä¸­å¿ƒ API å‘é€æ¶ˆæ¯ã€‚
    """

    def __init__(self, app_bot: FeishuAppBot, chat_id: str, proxy: str = None):
        """
        åˆå§‹åŒ–åŒ…è£…ç±»

        Args:
            app_bot: FeishuAppBot å®ä¾‹
            chat_id: ç¾¤èŠ ID
            proxy: ä»£ç† URLï¼ˆå¯é€‰ï¼‰
        """
        self.app_bot = app_bot
        self.chat_id = chat_id
        self.proxy = proxy

    def send_text(self, text: str) -> bool:
        """
        å‘é€æ–‡æœ¬æ¶ˆæ¯åˆ°ç¾¤èŠ

        Args:
            text: æ¶ˆæ¯æ–‡æœ¬

        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        # æ„å»ºé£ä¹¦å¯Œæ–‡æœ¬æ ¼å¼çš„æ–‡æœ¬æ¶ˆæ¯
        content = {"text": text}
        return self.app_bot.send_message_to_chat(
            self.chat_id,
            "text",
            content
        )

    def send_rich_text(self, title: str, content: list) -> bool:
        """
        å‘é€å¯Œæ–‡æœ¬æ¶ˆæ¯åˆ°ç¾¤èŠ

        Args:
            title: æ¶ˆæ¯æ ‡é¢˜
            content: å¯Œæ–‡æœ¬å†…å®¹ï¼ˆé£ä¹¦æ ¼å¼çš„äºŒç»´æ•°ç»„ï¼‰

        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        post_content = {
            "zh_cn": {
                "title": title,
                "content": content
            }
        }
        return self.app_bot.send_message_to_chat(
            self.chat_id,
            "post",
            post_content
        )

    def push_articles(self, articles: list[dict], batch_size: int = 10, with_feedback: bool = True) -> bool:
        """
        æ¨é€æ–‡ç« åˆ—è¡¨åˆ°ç¾¤èŠ

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            batch_size: æ¯æ‰¹æ¨é€çš„æ–‡ç« æ•°é‡
            with_feedback: æ˜¯å¦æ·»åŠ åé¦ˆæŒ‰é’®

        Returns:
            æ˜¯å¦å…¨éƒ¨æ¨é€æˆåŠŸ
        """
        if not articles:
            logger.info("æ²¡æœ‰æ–‡ç« éœ€è¦æ¨é€")
            return True

        # è¿‡æ»¤æœ‰æ•ˆæ–‡ç« 
        valid_articles = [
            a for a in articles
            if a.get('title', '').strip() and a.get('url', '').strip()
        ]

        if not valid_articles:
            logger.warning("æ‰€æœ‰æ–‡ç« éƒ½ç¼ºå°‘å¿…è¦å­—æ®µï¼ˆtitleæˆ–urlï¼‰")
            return False

        total_count = len(valid_articles)
        logger.info(f"å‡†å¤‡æ¨é€ {total_count} ç¯‡æ–‡ç« åˆ°é£ä¹¦ï¼ˆæ¯æ‰¹ {batch_size} ç¯‡ï¼‰")

        # åˆ†æ‰¹æ¨é€
        all_success = True

        for i in range(0, total_count, batch_size):
            batch = valid_articles[i:i + batch_size]
            batch_start = i + 1
            batch_end = min(i + batch_size, total_count)

            # æ„å»ºå¯Œæ–‡æœ¬å†…å®¹
            content = []
            for j, article in enumerate(batch, 1):
                title = article.get('title', '').strip()
                url = article.get('url', '').strip()

                if not title or not url:
                    continue

                # æ ‡é¢˜è¡Œï¼ˆå¸¦é“¾æ¥ï¼‰
                title_line = [
                    {"tag": "text", "text": f"{batch_start + j - 1}. "},
                    {"tag": "a", "text": title, "href": url}
                ]
                content.append(title_line)

                # æ‘˜è¦è¡Œ
                zh_summary = article.get('zh_summary', '').strip()
                summary = article.get('summary', '').strip()

                if zh_summary:
                    content.append([{"tag": "text", "text": f"   æ‘˜è¦: {zh_summary}"}])
                elif summary:
                    content.append([{"tag": "text", "text": f"   æ‘˜è¦: {summary}"}])

                # ç©ºè¡Œåˆ†éš”
                content.append([{"tag": "text", "text": ""}])

            title = f"ğŸ“š ä»Šæ—¥æ–‡ç« æ¨è ({batch_start}-{batch_end}/{total_count}ç¯‡)"

            success = self.send_rich_text(title, content)

            if not success:
                logger.error(f"ç¬¬ {i // batch_size + 1} æ‰¹æ¨é€å¤±è´¥")
                all_success = False

            # æ‰¹æ¬¡ä¹‹é—´é—´éš”
            if i + batch_size < total_count:
                time.sleep(1)

        if all_success:
            logger.info(f"å…¨éƒ¨ {total_count} ç¯‡æ–‡ç« æ¨é€æˆåŠŸ")
        else:
            logger.warning(f"éƒ¨åˆ†æ‰¹æ¬¡æ¨é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

        return all_success

    def send_interactive_card(self, card: dict) -> bool:
        """
        å‘é€äº¤äº’å¼å¡ç‰‡æ¶ˆæ¯åˆ°ç¾¤èŠ

        Args:
            card: é£ä¹¦å¡ç‰‡ JSON ç»“æ„

        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        if not card:
            logger.warning("å°è¯•å‘é€ç©ºå¡ç‰‡")
            return False

        return self.app_bot.send_message_to_chat(
            self.chat_id,
            "interactive",
            card
        )


class Scheduler:
    """
    å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
    Scheduled Task Scheduler
    
    è´Ÿè´£åè°ƒå„ä¸ªç»„ä»¶ï¼Œæ‰§è¡Œå®Œæ•´çš„çˆ¬å–-åˆ†æ-æ¨é€ä»»åŠ¡æµç¨‹ã€‚
    Coordinates all components to execute the complete fetch-analyze-push workflow.
    
    Attributes:
        config: å®Œæ•´é…ç½®å­—å…¸
        schedule_time: æ¯æ—¥æ‰§è¡Œæ—¶é—´ï¼ˆå¦‚ "09:00"ï¼‰
        timezone: æ—¶åŒºï¼ˆå¦‚ "Asia/Shanghai"ï¼‰
        _running: è°ƒåº¦å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ
    """
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        Initialize the scheduler
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰ç»„ä»¶çš„é…ç½®
                   Complete config dict containing all component configurations
        
        Examples:
            >>> config = load_config("config.yaml")
            >>> scheduler = Scheduler(config)
        """
        self.config = config
        
        # è°ƒåº¦é…ç½®
        schedule_config = config.get('schedule', {})
        self.schedule_time = schedule_config.get('time', '09:00')
        self.timezone = schedule_config.get('timezone', 'Asia/Shanghai')
        
        # æ–­ç‚¹ç»­ä¼ é…ç½®
        checkpoint_config = config.get('checkpoint', {})
        self.checkpoint_enabled = checkpoint_config.get('enabled', True)
        self.checkpoint_dir = checkpoint_config.get('dir', 'data/checkpoints')
        self.checkpoint_max_age = checkpoint_config.get('max_age_hours', 24)
        self.checkpoint_save_interval = checkpoint_config.get('save_interval', 10)
        
        # è¿è¡ŒçŠ¶æ€
        self._running = False
        
        logger.info(f"Scheduler initialized with schedule_time={self.schedule_time}, "
                   f"timezone={self.timezone}, checkpoint_enabled={self.checkpoint_enabled}")
    
    def _init_components(self) -> dict[str, Any]:
        """
        åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        Initialize all components
        
        Returns:
            åŒ…å«æ‰€æœ‰ç»„ä»¶å®ä¾‹çš„å­—å…¸
            Dict containing all component instances
        """
        components = {}
        
        # æ•°æ®åº“é…ç½®
        db_config = self.config.get('database', {})
        db_path = db_config.get('path', 'data/articles.db')
        
        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        db_dir = Path(db_path).parent
        db_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ä»“åº“
        components['repository'] = ArticleRepository(db_path)
        components['repository'].init_db()
        logger.info(f"ArticleRepository initialized with db_path={db_path}")
        
        # æ•°æ®æºé…ç½®
        sources_config = self.config.get('sources', {})
        
        # arXivè·å–å™¨
        arxiv_config = sources_config.get('arxiv', {})
        if arxiv_config.get('enabled', True):
            components['arxiv_fetcher'] = ArxivFetcher(arxiv_config)
            logger.info("ArxivFetcher initialized")
        
        # RSSè·å–å™¨
        rss_config = sources_config.get('rss', {})
        if rss_config.get('enabled', True):
            # æ·»åŠ ä»£ç†é…ç½®
            proxy_config = self.config.get('proxy', {})
            if proxy_config.get('enabled', False):
                rss_config['proxy'] = proxy_config.get('url')
            components['rss_fetcher'] = RSSFetcher(rss_config)
            logger.info("RSSFetcher initialized")
        
        # å†…å®¹å¤„ç†å™¨
        content_config = self.config.get('content', {})
        proxy_config = self.config.get('proxy', {})
        processor_config = {
            'max_content_length': content_config.get('max_length', 50000),
            'truncation_marker': content_config.get('truncation_marker', '\n\n... [å†…å®¹å·²æˆªæ–­]'),
        }
        if proxy_config.get('enabled', False):
            processor_config['proxy'] = proxy_config.get('url')
        components['content_processor'] = ContentProcessor(processor_config)
        logger.info("ContentProcessor initialized")
        
        # AIåˆ†æå™¨
        ai_config = self.config.get('ai', {})
        if ai_config.get('enabled', True):
            components['ai_analyzer'] = AIAnalyzer(ai_config)
            logger.info("AIAnalyzer initialized")
        
        # é£ä¹¦æœºå™¨äºº
        feishu_config = self.config.get('feishu', {})
        app_id = feishu_config.get('app_id', '')
        app_secret = feishu_config.get('app_secret', '')
        chat_id = feishu_config.get('chat_id', '')
        webhook_url = feishu_config.get('webhook_url', '')

        # ä¼˜å…ˆä½¿ç”¨åº”ç”¨ä¸­å¿ƒ APIï¼ˆFeishuAppBotï¼‰
        if app_id and app_secret and chat_id:
            proxy_url = None
            proxy_config = self.config.get('proxy', {})
            if proxy_config.get('enabled', False):
                proxy_url = proxy_config.get('url')
            # åˆ›å»º FeishuAppBot å®ä¾‹å¹¶åŒ…è£…ä¸ºå…¼å®¹ FeishuBot æ¥å£
            app_bot = FeishuAppBot(app_id, app_secret)
            # åˆ›å»ºåŒ…è£…ç±»ä»¥å…¼å®¹åŸæœ‰æ¥å£
            components['feishu_bot'] = FeishuAppBotWrapper(app_bot, chat_id, proxy_url)
            logger.info(f"FeishuAppBot initialized with chat_id={chat_id}")
        elif webhook_url:
            proxy_url = None
            proxy_config = self.config.get('proxy', {})
            if proxy_config.get('enabled', False):
                proxy_url = proxy_config.get('url')
            components['feishu_bot'] = FeishuBot(webhook_url, proxy=proxy_url)
            logger.info("FeishuBot initialized (webhook)")
        else:
            logger.warning("Feishu app_id/app_secret/chat_id or webhook_url not configured, push will be skipped")
        
        # åˆå§‹åŒ–é«˜çº§åŠŸèƒ½ç»„ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ADVANCED_FEATURES:
            data_sources_config = self.config.get('data_sources', {})
            
            # DBLP Fetcher
            dblp_config = data_sources_config.get('dblp', {})
            if dblp_config.get('enabled', False):
                components['dblp_fetcher'] = DBLPFetcher(dblp_config)
                logger.info("DBLPFetcher initialized")
            
            # NVD Fetcher
            nvd_config = data_sources_config.get('nvd', {})
            if nvd_config.get('enabled', False):
                components['nvd_fetcher'] = NVDFetcher(nvd_config)
                logger.info("NVDFetcher initialized")
            
            # KEV Fetcher
            kev_config = data_sources_config.get('kev', {})
            if kev_config.get('enabled', False):
                components['kev_fetcher'] = KEVFetcher(kev_config)
                logger.info("KEVFetcher initialized")
            
            # HuggingFace Fetcher
            hf_config = data_sources_config.get('huggingface', {})
            if hf_config.get('enabled', False):
                components['huggingface_fetcher'] = HuggingFaceFetcher(hf_config)
                logger.info("HuggingFaceFetcher initialized")
            
            # Hunyuan Research Fetcher (è…¾è®¯æ··å…ƒç ”ç©¶)
            hunyuan_config = data_sources_config.get('hunyuan', {})
            if hunyuan_config.get('enabled', False):
                components['hunyuan_fetcher'] = HunyuanFetcher(hunyuan_config)
                logger.info("HunyuanFetcher initialized")
            
            # GitHub Fetcher (çƒ­é—¨é¡¹ç›®)
            github_config = data_sources_config.get('github', {})
            if github_config.get('enabled', False):
                components['github_fetcher'] = GitHubFetcher(github_config)
                logger.info("GitHubFetcher initialized")
            
            # PWC Fetcher
            pwc_config = data_sources_config.get('pwc', {})
            if pwc_config.get('enabled', False):
                components['pwc_fetcher'] = PWCFetcher(pwc_config)
                logger.info("PWCFetcher initialized")
            
            # Blog Fetcher
            blogs_config = data_sources_config.get('blogs', {})
            if blogs_config.get('enabled', False):
                components['blog_fetcher'] = BlogFetcher(blogs_config)
                logger.info("BlogFetcher initialized")
            
            # Anthropic Red Team Fetcher
            anthropic_red_config = data_sources_config.get('anthropic_red', {})
            if anthropic_red_config.get('enabled', False):
                components['anthropic_red_fetcher'] = AnthropicRedFetcher(anthropic_red_config)
                logger.info("AnthropicRedFetcher initialized")
            
            # Atum Blog Fetcher
            atum_blog_config = data_sources_config.get('atum_blog', {})
            if atum_blog_config.get('enabled', False):
                components['atum_blog_fetcher'] = AtumBlogFetcher(atum_blog_config)
                logger.info("AtumBlogFetcher initialized")
            
            # Vulnerability Filter
            vuln_filter_config = self.config.get('vulnerability_filter', {})
            if vuln_filter_config.get('enabled', False) and 'ai_analyzer' in components:
                components['vulnerability_filter'] = VulnerabilityFilter(
                    vuln_filter_config, 
                    components['ai_analyzer']
                )
                logger.info("VulnerabilityFilter initialized")
            
            # Priority Scorer
            priority_config = self.config.get('priority_scoring', {})
            if priority_config.get('enabled', False):
                components['priority_scorer'] = PriorityScorer(
                    priority_config,
                    components.get('ai_analyzer')
                )
                logger.info("PriorityScorer initialized")
            
            # Tiered Pusher
            tiered_push_config = self.config.get('tiered_push', {})
            if tiered_push_config.get('enabled', False) and 'feishu_bot' in components:
                components['tiered_pusher'] = TieredPusher(
                    tiered_push_config,
                    components['feishu_bot'],
                    components.get('ai_analyzer')
                )
                logger.info("TieredPusher initialized")
            
            # Smart Selector
            smart_selector_config = self.config.get('smart_selector', {})
            if smart_selector_config.get('enabled', True):  # é»˜è®¤å¯ç”¨
                from src.pushers.smart_selector import SmartSelector
                components['smart_selector'] = SmartSelector(
                    smart_selector_config,
                    components.get('ai_analyzer')
                )
                logger.info("SmartSelector initialized")
        
        # é£ä¹¦å¤šç»´è¡¨æ ¼ï¼ˆç”¨äºæ•°æ®å¯è§†åŒ–ï¼‰
        if BITABLE_AVAILABLE:
            bitable_config = self.config.get('feishu_bitable', {})
            if bitable_config.get('enabled', False) and bitable_config.get('app_id'):
                try:
                    components['feishu_bitable'] = FeishuBitable(bitable_config)
                    logger.info("FeishuBitable initialized")
                except Exception as e:
                    logger.warning(f"Failed to initialize FeishuBitable: {e}")

        # ç”¨æˆ·åé¦ˆç³»ç»Ÿ
        feedback_config = self.config.get('feedback', {})
        if feedback_config.get('enabled', False):
            try:
                from src.feedback.feedback_handler import FeedbackHandler
                db_path = feedback_config.get('db_path', 'data/articles.db')
                components['feedback_handler'] = FeedbackHandler(db_path)
                logger.info("FeedbackHandler initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize FeedbackHandler: {e}")

        # PDFç¿»è¯‘æœåŠ¡
        pdf_translation_config = self.config.get('pdf_translation', {})
        if pdf_translation_config.get('enabled', False):
            try:
                from src.bots.feishu_pdf_translator import FeishuPDFTranslationService
                components['pdf_translation_service'] = FeishuPDFTranslationService(pdf_translation_config)
                logger.info("FeishuPDFTranslationService initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize PDF translation service: {e}")

        return components
    
    def _cleanup_components(self, components: dict[str, Any]):
        """
        æ¸…ç†ç»„ä»¶èµ„æº
        Cleanup component resources
        
        Args:
            components: ç»„ä»¶å­—å…¸
        """
        # å…³é—­å†…å®¹å¤„ç†å™¨ï¼ˆé‡Šæ”¾Playwrightèµ„æºï¼‰
        if 'content_processor' in components:
            try:
                components['content_processor'].close()
            except Exception as e:
                logger.warning(f"Error closing content_processor: {e}")
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        if 'repository' in components:
            try:
                components['repository'].close()
            except Exception as e:
                logger.warning(f"Error closing repository: {e}")
    
    def _push_error_report(
        self, 
        feishu_bot, 
        errors: list[dict], 
        duration: float
    ):
        """
        æ¨é€é”™è¯¯æ±‡æ€»æŠ¥å‘Šåˆ°é£ä¹¦
        
        Args:
            feishu_bot: é£ä¹¦æœºå™¨äººå®ä¾‹
            errors: é”™è¯¯åˆ—è¡¨ [{'source': 'xxx', 'error': 'xxx'}, ...]
            duration: ä»»åŠ¡è€—æ—¶ï¼ˆç§’ï¼‰
        """
        if not errors:
            return
        
        try:
            # æ„å»ºé”™è¯¯æŠ¥å‘Š
            error_lines = []
            for err in errors:
                source = err.get('source', 'Unknown')
                error_msg = err.get('error', 'Unknown error')
                # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
                if len(error_msg) > 200:
                    error_msg = error_msg[:200] + '...'
                error_lines.append(f"â€¢ {source}: {error_msg}")
            
            report = f"""âš ï¸ æ•°æ®æºæŠ“å–å¼‚å¸¸æŠ¥å‘Š

ä»»åŠ¡è€—æ—¶: {duration:.1f}s
å¼‚å¸¸æ•°é‡: {len(errors)}

å¼‚å¸¸è¯¦æƒ…:
{chr(10).join(error_lines)}"""
            
            # ä½¿ç”¨é£ä¹¦æœºå™¨äººå‘é€
            feishu_bot.send_text(report)
            logger.info(f"é”™è¯¯æŠ¥å‘Šå·²æ¨é€ï¼Œå…± {len(errors)} ä¸ªå¼‚å¸¸")
            
        except Exception as e:
            logger.error(f"æ¨é€é”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _get_existing_urls(self, repository: ArticleRepository) -> set[str]:
        """
        è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ‰€æœ‰æ–‡ç«  URL
        Get all existing article URLs from database
        
        ç”¨äºåœ¨æŠ“å–é˜¶æ®µå¿«é€Ÿè¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« ï¼Œé¿å…é‡å¤å¤„ç†ã€‚
        
        Args:
            repository: æ–‡ç« ä»“åº“å®ä¾‹
        
        Returns:
            å·²å­˜åœ¨çš„ URL é›†åˆ
        """
        try:
            conn = repository._get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT url FROM articles")
            rows = cursor.fetchall()
            return {row['url'] for row in rows}
        except Exception as e:
            logger.warning(f"è·å–å·²å­˜åœ¨ URL å¤±è´¥: {e}")
            return set()
    
    def run_task(self):
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬å–-åˆ†æ-æ¨é€ä»»åŠ¡
        Execute the complete fetch-analyze-push task
        
        ä»»åŠ¡æµç¨‹ï¼š
        1. ä»arXivè·å–è®ºæ–‡
        2. ä»RSSè®¢é˜…æºè·å–æ–‡ç« ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        3. æ£€æŸ¥æ•°æ®åº“å»é‡
        4. å¤„ç†å†…å®¹ï¼ˆè·å–HTMLå¹¶è½¬æ¢ä¸ºMarkdownï¼‰
        5. AIåˆ†æï¼ˆç”Ÿæˆæ‘˜è¦ã€åˆ†ç±»ã€ç¿»è¯‘ï¼‰
        6. ä¿å­˜åˆ°æ•°æ®åº“
        7. è·å–æœªæ¨é€æ–‡ç« å¹¶æ¨é€åˆ°é£ä¹¦
        8. æ ‡è®°æ–‡ç« ä¸ºå·²æ¨é€
        
        Task workflow:
        1. Fetch papers from arXiv
        2. Fetch articles from RSS feeds (with checkpoint/resume support)
        3. Check for duplicates in database
        4. Process content (fetch HTML and convert to Markdown)
        5. AI analysis (generate summary, category, translation)
        6. Save to database
        7. Get unpushed articles and push to Feishu
        8. Mark articles as pushed
        
        **éªŒè¯: éœ€æ±‚ 7.3, 7.4**
        """
        start_time = datetime.now()
        logger.info(f"=== Task started at {start_time.isoformat()} ===")
        
        components = None
        checkpoint_manager = None
        fetch_errors: list[dict] = []  # æ”¶é›†æŠ“å–é”™è¯¯
        
        try:
            # åˆå§‹åŒ–ç»„ä»¶
            components = self._init_components()
            repository = components['repository']
            
            # åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
            if CHECKPOINT_AVAILABLE and self.checkpoint_enabled:
                checkpoint_manager = CheckpointManager(
                    checkpoint_dir=self.checkpoint_dir,
                    max_age_hours=self.checkpoint_max_age,
                    auto_save_interval=self.checkpoint_save_interval
                )
                checkpoint_manager.cleanup_old_checkpoints()
                logger.info("æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å·²åˆå§‹åŒ–")
            
            all_articles = []
            
            # é¢„å…ˆè·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„ URL é›†åˆï¼ˆç”¨äºæ‰€æœ‰æ•°æ®æºçš„å¿«é€Ÿå»é‡ï¼‰
            existing_urls = self._get_existing_urls(repository)
            logger.info(f"æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_urls)} ç¯‡æ–‡ç« ")
            
            # æ­¥éª¤1: ä»arXivè·å–è®ºæ–‡
            if 'arxiv_fetcher' in components:
                logger.info("Step 1: Fetching papers from arXiv...")
                try:
                    arxiv_fetcher = components['arxiv_fetcher']
                    papers = arxiv_fetcher.fetch_papers()
                    
                    # åº”ç”¨å…³é”®è¯è¿‡æ»¤
                    if arxiv_fetcher.keywords:
                        papers = arxiv_fetcher.filter_by_keywords(papers)
                    
                    # è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡ç« 
                    new_papers = [p for p in papers if p.get('url', '') not in existing_urls]
                    logger.info(f"arXiv: æ–°å¢ {len(new_papers)} ç¯‡ï¼Œè·³è¿‡ {len(papers) - len(new_papers)} ç¯‡å·²å­˜åœ¨")
                    all_articles.extend(new_papers)
                except Exception as e:
                    logger.error(f"Error fetching arXiv papers: {e}")
                    fetch_errors.append({'source': 'arXiv', 'error': str(e)})
            
            # æ­¥éª¤2: ä»RSSè®¢é˜…æºè·å–æ–‡ç« ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
            if 'rss_fetcher' in components:
                logger.info("Step 2: Fetching articles from RSS feeds...")
                try:
                    rss_fetcher = components['rss_fetcher']
                    opml_path = rss_fetcher.opml_path
                    
                    if opml_path and Path(opml_path).exists():
                        all_urls = rss_fetcher.parse_opml(opml_path)
                        
                        # ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
                        if checkpoint_manager:
                            fetch_checkpoint = checkpoint_manager.start_fetch(all_urls)
                            pending_urls = checkpoint_manager.get_pending_feeds(all_urls)
                            
                            # å…ˆåŠ è½½å·²æŠ“å–çš„æ–‡ç« ï¼ˆè¿‡æ»¤æ‰å·²å­˜åœ¨çš„ï¼‰
                            existing_articles = checkpoint_manager.get_all_fetched_articles()
                            if existing_articles:
                                new_from_checkpoint = [
                                    a for a in existing_articles 
                                    if a.get('url', '') not in existing_urls
                                ]
                                all_articles.extend(new_from_checkpoint)
                                logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ {len(new_from_checkpoint)} ç¯‡æ–°æ–‡ç« ï¼ˆè·³è¿‡ {len(existing_articles) - len(new_from_checkpoint)} ç¯‡å·²å­˜åœ¨ï¼‰")
                            
                            if pending_urls:
                                logger.info(f"å¾…æŠ“å–è®¢é˜…æº: {len(pending_urls)}/{len(all_urls)}")
                                
                                # å®šä¹‰å›è°ƒå‡½æ•°
                                def on_feed_complete(url, name, articles):
                                    checkpoint_manager.mark_feed_completed(url, articles, name)
                                
                                def on_feed_error(url, error):
                                    checkpoint_manager.mark_feed_failed(url, error)
                                
                                # æŠ“å–å‰©ä½™è®¢é˜…æº
                                feeds_result = rss_fetcher.fetch_all_feeds(
                                    pending_urls,
                                    on_feed_complete=on_feed_complete,
                                    on_feed_error=on_feed_error
                                )
                                
                                # åˆå¹¶æ–°æŠ“å–çš„æ–‡ç« ï¼ˆè¿‡æ»¤æ‰å·²å­˜åœ¨çš„ï¼‰
                                new_count = 0
                                skip_count = 0
                                for feed_name, articles in feeds_result.items():
                                    for article in articles:
                                        if article.get('url', '') not in existing_urls:
                                            all_articles.append(article)
                                            new_count += 1
                                        else:
                                            skip_count += 1
                                
                                logger.info(f"RSS æŠ“å–: æ–°å¢ {new_count} ç¯‡ï¼Œè·³è¿‡ {skip_count} ç¯‡å·²å­˜åœ¨")
                                
                                # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
                                checkpoint_manager.save_fetch_checkpoint()
                            else:
                                logger.info("æ‰€æœ‰è®¢é˜…æºå·²åœ¨ä¹‹å‰çš„è¿è¡Œä¸­å®Œæˆ")
                            
                            checkpoint_manager.complete_fetch()
                        else:
                            # ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œç›´æ¥æŠ“å–ï¼ˆä½†ä»ç„¶è¿‡æ»¤å·²å­˜åœ¨çš„ï¼‰
                            feeds_result = rss_fetcher.fetch_all_feeds(all_urls)
                            new_count = 0
                            skip_count = 0
                            for feed_name, articles in feeds_result.items():
                                for article in articles:
                                    if article.get('url', '') not in existing_urls:
                                        all_articles.append(article)
                                        new_count += 1
                                    else:
                                        skip_count += 1
                            
                            logger.info(f"RSS æŠ“å–: æ–°å¢ {new_count} ç¯‡ï¼Œè·³è¿‡ {skip_count} ç¯‡å·²å­˜åœ¨")
                        
                        logger.info(f"RSSæŠ“å–å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
                    else:
                        logger.warning(f"OPML file not found: {opml_path}")
                except Exception as e:
                    logger.error(f"Error fetching RSS articles: {e}")
                    fetch_errors.append({'source': 'RSS', 'error': str(e)})
                    # ä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸‹æ¬¡æ¢å¤
                    if checkpoint_manager:
                        checkpoint_manager.save_fetch_checkpoint()
            
            # æ­¥éª¤2.1: ä»æ–°æ•°æ®æºè·å–æ–‡ç« ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
            # æ³¨æ„ï¼šexisting_urls å·²åœ¨å‰é¢è·å–
            vulnerability_articles = []  # æ¼æ´ç±»æ–‡ç« å•ç‹¬å¤„ç†
            
            # DBLP - å®‰å…¨å››å¤§é¡¶ä¼š
            if 'dblp_fetcher' in components:
                logger.info("Step 2.1a: Fetching papers from DBLP...")
                try:
                    result = components['dblp_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"DBLP: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching DBLP papers: {e}")
                    fetch_errors.append({'source': 'DBLP', 'error': str(e)})
            
            # NVD - æ¼æ´æ•°æ®åº“
            if 'nvd_fetcher' in components:
                logger.info("Step 2.1b: Fetching CVEs from NVD...")
                try:
                    result = components['nvd_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        vulnerability_articles.extend(new_items)
                        logger.info(f"NVD: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching NVD CVEs: {e}")
                    fetch_errors.append({'source': 'NVD', 'error': str(e)})
            
            # KEV - CISA åœ¨é‡åˆ©ç”¨æ¼æ´
            if 'kev_fetcher' in components:
                logger.info("Step 2.1c: Fetching KEV entries from CISA...")
                try:
                    result = components['kev_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        vulnerability_articles.extend(new_items)
                        logger.info(f"KEV: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching KEV entries: {e}")
                    fetch_errors.append({'source': 'KEV', 'error': str(e)})
            
            # HuggingFace Papers
            if 'huggingface_fetcher' in components:
                logger.info("Step 2.1d: Fetching papers from HuggingFace...")
                try:
                    result = components['huggingface_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"HuggingFace: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching HuggingFace papers: {e}")
                    fetch_errors.append({'source': 'HuggingFace', 'error': str(e)})
            
            # Papers With Code
            if 'pwc_fetcher' in components:
                logger.info("Step 2.1e: Fetching papers from Papers With Code...")
                try:
                    result = components['pwc_fetcher'].fetch()
                    if result.error:
                        fetch_errors.append({'source': 'PWC', 'error': result.error})
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"PWC: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching PWC papers: {e}")
                    fetch_errors.append({'source': 'PWC', 'error': str(e)})
            
            # å¤§å‚åšå®¢
            if 'blog_fetcher' in components:
                logger.info("Step 2.1f: Fetching articles from tech blogs...")
                try:
                    result = components['blog_fetcher'].fetch()
                    if result.error:
                        fetch_errors.append({'source': 'Blogs', 'error': result.error})
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"Blogs: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching blog articles: {e}")
                    fetch_errors.append({'source': 'Blogs', 'error': str(e)})
            
            # è…¾è®¯æ··å…ƒç ”ç©¶
            if 'hunyuan_fetcher' in components:
                logger.info("Step 2.1g: Fetching articles from Hunyuan Research...")
                try:
                    result = components['hunyuan_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"Hunyuan: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching Hunyuan Research articles: {e}")
                    fetch_errors.append({'source': 'Hunyuan', 'error': str(e)})
            
            # GitHub çƒ­é—¨é¡¹ç›®
            if 'github_fetcher' in components:
                logger.info("Step 2.1h: Fetching trending projects from GitHub...")
                try:
                    projects = components['github_fetcher'].fetch()
                    if projects:
                        new_items = [p for p in projects if p.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"GitHub: æ–°å¢ {len(new_items)} ä¸ªé¡¹ç›®ï¼Œè·³è¿‡ {len(projects) - len(new_items)} ä¸ªå·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching GitHub projects: {e}")
                    fetch_errors.append({'source': 'GitHub', 'error': str(e)})
            
            # Anthropic Red Team
            if 'anthropic_red_fetcher' in components:
                logger.info("Step 2.1i: Fetching articles from Anthropic Red Team...")
                try:
                    result = components['anthropic_red_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"Anthropic Red: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching Anthropic Red Team articles: {e}")
                    fetch_errors.append({'source': 'Anthropic Red', 'error': str(e)})
            
            # Atum Blog
            if 'atum_blog_fetcher' in components:
                logger.info("Step 2.1j: Fetching articles from Atum Blog...")
                try:
                    result = components['atum_blog_fetcher'].fetch()
                    if result.items:
                        new_items = [a for a in result.items if a.get('url', '') not in existing_urls]
                        all_articles.extend(new_items)
                        logger.info(f"Atum Blog: æ–°å¢ {len(new_items)} ç¯‡ï¼Œè·³è¿‡ {len(result.items) - len(new_items)} ç¯‡å·²å­˜åœ¨")
                except Exception as e:
                    logger.error(f"Error fetching Atum Blog articles: {e}")
                    fetch_errors.append({'source': 'Atum Blog', 'error': str(e)})
            
            # æ­¥éª¤2.2: æ¼æ´è¿‡æ»¤ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
            if vulnerability_articles and 'vulnerability_filter' in components:
                logger.info("Step 2.2: Filtering vulnerabilities...")
                try:
                    vuln_filter = components['vulnerability_filter']
                    filter_results = vuln_filter.filter_vulnerabilities(vulnerability_articles)
                    
                    # åªä¿ç•™é€šè¿‡è¿‡æ»¤çš„æ¼æ´
                    passed_vulns = [r.vulnerability for r in filter_results if r.passed]
                    filtered_count = len(vulnerability_articles) - len(passed_vulns)
                    
                    all_articles.extend(passed_vulns)
                    logger.info(f"Vulnerability filter: {len(passed_vulns)} passed, {filtered_count} filtered")
                except Exception as e:
                    logger.error(f"Error filtering vulnerabilities: {e}")
                    # å¦‚æœè¿‡æ»¤å¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰æ¼æ´
                    all_articles.extend(vulnerability_articles)
            else:
                # æ²¡æœ‰æ¼æ´è¿‡æ»¤å™¨ï¼Œç›´æ¥æ·»åŠ æ‰€æœ‰æ¼æ´
                all_articles.extend(vulnerability_articles)
            
            logger.info(f"Total new articles to process: {len(all_articles)}")
            
            # æ­¥éª¤3: æ£€æŸ¥æ•°æ®åº“å»é‡ï¼ˆäºŒæ¬¡ç¡®è®¤ï¼Œä¸»è¦ç”¨äºæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼‰
            logger.info("Step 3: Checking for duplicates (title similarity)...")
            new_articles = []

            # æ‰¹é‡å»é‡ä¼˜åŒ–ï¼šæ¯100ç¯‡æ‰“å°è¿›åº¦
            batch_size = 100
            total = len(all_articles)

            for i, article in enumerate(all_articles):
                url = article.get('url', '')
                title = article.get('title', '')

                # è¿›åº¦æ—¥å¿—
                if i % batch_size == 0 or i == total - 1:
                    logger.info(f"å»é‡è¿›åº¦: {i+1}/{total} ({(i+1)/total*100:.1f}%)")

                # URLå»é‡
                if repository.exists_by_url(url):
                    logger.debug(f"Skipping duplicate URL: {url}")
                    continue

                # æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼ˆå·²ç¦ç”¨ï¼šæ€§èƒ½ç“¶é¢ˆï¼‰
                # similar = repository.find_similar_by_title(title)
                # if similar:
                #     logger.debug(f"Skipping similar title: {title}")
                #     continue
                # è·³è¿‡æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥ä»¥æå‡æ€§èƒ½ï¼ˆ6282ç¯‡å·²æœ‰æ–‡ç«  Ã— 17017ç¯‡æ–°æ–‡ç«  = 1.07äº¿æ¬¡æ¯”è¾ƒï¼‰
                pass

                new_articles.append(article)
            
            logger.info(f"New articles after deduplication: {len(new_articles)}")
            
            if not new_articles:
                logger.info("No new articles to process")
            else:
                # æ­¥éª¤4 & 5 & 6: å¤„ç†å†…å®¹ã€AIåˆ†æã€ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
                content_processor = components.get('content_processor')
                ai_analyzer = components.get('ai_analyzer')
                
                # åˆå§‹åŒ–å¤„ç†é˜¶æ®µæ£€æŸ¥ç‚¹
                if checkpoint_manager:
                    checkpoint_manager.start_process(new_articles)
                    pending_articles = checkpoint_manager.get_pending_articles(new_articles)
                    
                    # åŠ è½½å·²å¤„ç†çš„æ–‡ç« 
                    processed_from_checkpoint = checkpoint_manager.get_processed_articles()
                    if processed_from_checkpoint:
                        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ {len(processed_from_checkpoint)} ç¯‡å·²å¤„ç†æ–‡ç« ")
                        # è¿™äº›æ–‡ç« å·²ç»ä¿å­˜åˆ°æ•°æ®åº“äº†ï¼Œä¸éœ€è¦é‡æ–°å¤„ç†
                    
                    if not pending_articles:
                        logger.info("æ‰€æœ‰æ–‡ç« å·²åœ¨ä¹‹å‰çš„è¿è¡Œä¸­å¤„ç†å®Œæˆ")
                        new_articles = []
                    else:
                        logger.info(f"å¾…å¤„ç†æ–‡ç« : {len(pending_articles)}/{len(new_articles)}")
                        new_articles = pending_articles
                
                processed_count = 0
                processed_lock = Lock()
                total = len(new_articles)
                
                def process_single_article(article):
                    nonlocal processed_count
                    try:
                        url = article.get('url', '')
                        title = article.get('title', '')
                        source_type = article.get('source_type', '')
                        
                        # æ­¥éª¤4: å¤„ç†å†…å®¹
                        if source_type == 'rss' and content_processor:
                            content = content_processor.process_article(url)
                            if content:
                                article['content'] = content
                            else:
                                article['content'] = ''
                        elif not article.get('content'):
                            content_parts = []
                            if article.get('short_description'):
                                content_parts.append(article['short_description'])
                            if article.get('description'):
                                content_parts.append(article['description'])
                            if article.get('required_action'):
                                content_parts.append(f"Required Action: {article['required_action']}")
                            if article.get('summary'):
                                content_parts.append(article['summary'])
                            
                            if content_parts:
                                article['content'] = '\n\n'.join(content_parts)
                        
                        # æ­¥éª¤5: AIåˆ†æ
                        if ai_analyzer and article.get('content'):
                            analysis_result = ai_analyzer.analyze_article(
                                title, 
                                article.get('content', '')
                            )
                            article['summary'] = analysis_result.get('summary', '')
                            article['category'] = analysis_result.get('category', 'å…¶ä»–')
                            article['zh_summary'] = analysis_result.get('zh_summary', '')
                        
                        # æ­¥éª¤6: ä¿å­˜åˆ°æ•°æ®åº“
                        article['fetched_at'] = datetime.now().isoformat()
                        article['is_pushed'] = False
                        
                        article_id = repository.save_article(article)
                        article['id'] = article_id
                        
                        with processed_lock:
                            nonlocal processed_count
                            processed_count += 1
                            current = processed_count
                            if current % 10 == 0 or current == total:
                                logger.info(f"å¤„ç†è¿›åº¦: {current}/{total} ({current/total*100:.1f}%) - {title[:40]}...")
                        
                        # æ ‡è®°æ–‡ç« å¤„ç†å®Œæˆ
                        if checkpoint_manager:
                            checkpoint_manager.mark_article_processed(article)
                            
                        return True
                        
                    except Exception as e:
                        logger.error(f"Error processing article {article.get('title', 'unknown')}: {e}")
                        if checkpoint_manager:
                            checkpoint_manager.mark_article_failed(article.get('url', ''), str(e))
                        return False
                
                # ä½¿ç”¨10çº¿ç¨‹å¹¶å‘å¤„ç†
                max_workers = min(10, total)
                logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {total} ç¯‡æ–‡ç« ï¼ˆ{max_workers}çº¿ç¨‹ï¼‰...")
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(process_single_article, article) for article in new_articles]
                    for future in as_completed(futures):
                        future.result()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                
                # å®Œæˆå¤„ç†é˜¶æ®µ
                if checkpoint_manager:
                    checkpoint_manager.complete_process()
                    checkpoint_manager.save_process_checkpoint()
                
                logger.info(f"Processed and saved {processed_count} articles")
            
            # æ­¥éª¤7: è·å–æœªæ¨é€æ–‡ç« å¹¶æ¨é€åˆ°é£ä¹¦
            logger.info("Step 7: Pushing articles to Feishu...")
            if 'feishu_bot' in components:
                feishu_bot = components['feishu_bot']
                unpushed_articles = repository.get_unpushed_articles()
                
                if unpushed_articles:
                    logger.info(f"Found {len(unpushed_articles)} unpushed articles")
                    
                    # æ™ºèƒ½ç­›é€‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    articles_to_push = unpushed_articles
                    if 'smart_selector' in components:
                        logger.info("Using smart selector to filter articles...")
                        smart_selector = components['smart_selector']
                        articles_to_push = smart_selector.select_articles(unpushed_articles)
                        logger.info(f"Smart selector: {len(articles_to_push)}/{len(unpushed_articles)} articles selected")
                    
                    # ä½¿ç”¨åˆ†çº§æ¨é€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if 'tiered_pusher' in components and 'priority_scorer' in components:
                        logger.info("Using tiered push with priority scoring...")
                        try:
                            priority_scorer = components['priority_scorer']
                            tiered_pusher = components['tiered_pusher']
                            
                            # è¯„åˆ†
                            scored_articles = priority_scorer.score_articles(articles_to_push)
                            
                            # æ’åº
                            sorted_articles = priority_scorer.sort_by_priority(scored_articles)
                            
                            # åˆ†çº§
                            tiered_articles = tiered_pusher.categorize_articles(sorted_articles)
                            
                            # æ¨é€
                            success = tiered_pusher.push_tiered(tiered_articles)
                        except Exception as e:
                            logger.error(f"Tiered push failed, falling back to standard push: {e}")
                            success = feishu_bot.push_articles(articles_to_push)
                    else:
                        # æ ‡å‡†æ¨é€
                        success = feishu_bot.push_articles(articles_to_push)
                    
                    if success:
                        # æ­¥éª¤8: æ ‡è®°å·²æ¨é€çš„æ–‡ç« ï¼ˆåªæ ‡è®°å®é™…æ¨é€çš„ï¼‰
                        article_ids = [a['id'] for a in articles_to_push if a.get('id')]
                        repository.mark_as_pushed(article_ids)
                        logger.info(f"Marked {len(article_ids)} articles as pushed")
                        
                        # æ­¥éª¤9: åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
                        if 'feishu_bitable' in components:
                            logger.info("Step 9: Syncing articles to Feishu Bitable...")
                            try:
                                bitable = components['feishu_bitable']
                                # æ›´æ–°æ¨é€çŠ¶æ€ååŒæ­¥
                                for article in articles_to_push:
                                    article['is_pushed'] = True
                                sync_count = bitable.batch_add_records(articles_to_push)
                                logger.info(f"Synced {sync_count} articles to Feishu Bitable")
                            except Exception as e:
                                logger.error(f"Failed to sync to Bitable: {e}")
                        
                        # æ­¥éª¤10: è¯é¢˜èšåˆä¸é£ä¹¦æ–‡æ¡£å‘å¸ƒ
                        topic_config = self.config.get('topic_aggregation', {})
                        if topic_config.get('enabled', False):
                            logger.info("Step 10: Running topic aggregation...")
                            try:
                                self._run_topic_aggregation(articles_to_push, components)
                            except Exception as e:
                                logger.error(f"Topic aggregation failed: {e}")
                    else:
                        logger.error("Failed to push articles to Feishu")
                        # æ¨é€å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ä»¥ä¿ç•™æ£€æŸ¥ç‚¹
                        raise RuntimeError("Feishu push failed, checkpoint preserved for retry")
                else:
                    logger.info("No unpushed articles to push")
            else:
                logger.warning("FeishuBot not configured, skipping push")
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            logger.info(f"=== Task completed at {end_time.isoformat()} "
                       f"(duration: {duration:.2f}s) ===")
            
            # æ¨é€é”™è¯¯æ±‡æ€»æŠ¥å‘Š
            if fetch_errors and 'feishu_bot' in components:
                self._push_error_report(components['feishu_bot'], fetch_errors, duration)
            
            # ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œæ¸…ç†æ£€æŸ¥ç‚¹
            if checkpoint_manager:
                checkpoint_manager.clear_checkpoints()
                logger.info("æ£€æŸ¥ç‚¹å·²æ¸…ç†")
            
        except Exception as e:
            logger.error(f"Task failed with error: {e}", exc_info=True)
            # ä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸‹æ¬¡æ¢å¤
            if checkpoint_manager:
                checkpoint_manager.save_fetch_checkpoint()
                checkpoint_manager.save_process_checkpoint()
                logger.info("æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ–­ç‚¹æ¢å¤")
            raise
        finally:
            # æ¸…ç†èµ„æº
            if components:
                self._cleanup_components(components)
    
    def _run_topic_aggregation(
        self, 
        articles: list[dict], 
        components: dict
    ):
        """
        è¿è¡Œè¯é¢˜èšåˆå¹¶å‘å¸ƒåˆ°é£ä¹¦æ–‡æ¡£
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            components: ç»„ä»¶å­—å…¸
        """
        from src.aggregation.topic_aggregation_system import TopicAggregationSystem
        from src.models import Article
        
        # è·å–è¯é¢˜èšåˆé…ç½®
        topic_config = self.config.get('topic_aggregation', {})
        ai_config = self.config.get('ai', {})
        bitable_config = self.config.get('feishu_bitable', {})
        
        # æ„å»ºè¯é¢˜èšåˆç³»ç»Ÿé…ç½®
        system_config = {
            'quality_filter': {
                'blacklist_domains': topic_config.get('blacklist_domains', []),
                'trusted_sources': topic_config.get('trusted_sources', []),
            },
            'aggregation_engine': {
                'similarity_threshold': topic_config.get('similarity_threshold', 0.7),
                'aggregation_threshold': topic_config.get('aggregation_threshold', 3),
                'time_window_days': topic_config.get('time_window_days', 7),
                'title_weight': topic_config.get('title_weight', 0.6),
                'keyword_weight': topic_config.get('keyword_weight', 0.4),
                'use_ai_similarity': topic_config.get('use_ai_similarity', False),
            },
            'synthesis_generator': {},
            'doc_publisher': {
                'app_id': bitable_config.get('app_id', ''),
                'app_secret': bitable_config.get('app_secret', ''),
                'folder_token': topic_config.get('folder_token', ''),
                'backup_dir': 'data/doc_backups',
            },
            'rss_generator': {
                'output_path': 'data/knowledge_feed.xml',
            },
            'ai': ai_config,
        }
        
        # è½¬æ¢æ–‡ç« æ ¼å¼
        article_objects = []
        for a in articles:
            try:
                article = Article(
                    title=a.get('title', ''),
                    url=a.get('url', ''),
                    source=a.get('source', ''),
                    source_type=a.get('source_type', ''),
                    content=a.get('content', ''),
                    summary=a.get('summary', ''),
                    zh_summary=a.get('zh_summary', ''),
                    category=a.get('category', ''),
                    fetched_at=a.get('fetched_at', ''),
                )
                article_objects.append(article)
            except Exception as e:
                logger.warning(f"Failed to convert article: {e}")
        
        if not article_objects:
            logger.warning("No valid articles for topic aggregation")
            return
        
        # è¿è¡Œè¯é¢˜èšåˆ
        try:
            system = TopicAggregationSystem(system_config)
            result = system.run(
                article_objects,
                publish_to_feishu=True,
                generate_rss=True
            )
            
            stats = result.get('stats', {})
            logger.info(
                f"Topic aggregation completed: "
                f"clusters={stats.get('clusters_count', 0)}, "
                f"syntheses={stats.get('syntheses_count', 0)}, "
                f"published={stats.get('published_count', 0)}"
            )
            
            # å¦‚æœæœ‰ç»¼è¿°å‘å¸ƒæˆåŠŸï¼Œå‘é€é€šçŸ¥
            if stats.get('published_count', 0) > 0:
                publish_results = result.get('publish_results', [])
                for pr in publish_results:
                    if pr.success and pr.document_url:
                        # å‘é€é£ä¹¦é€šçŸ¥
                        if 'feishu_bot' in components:
                            msg = f"ğŸ“„ è¯é¢˜ç»¼è¿°å·²å‘å¸ƒ: {pr.document_url}"
                            components['feishu_bot'].send_text(msg)
                            
        except Exception as e:
            logger.error(f"Topic aggregation error: {e}", exc_info=True)
    
    def start(self):
        """
        å¯åŠ¨å®šæ—¶è°ƒåº¦
        Start scheduled execution
        
        æ ¹æ®é…ç½®çš„æ—¶é—´æ¯å¤©æ‰§è¡Œä»»åŠ¡ã€‚
        Executes task daily at the configured time.
        
        **éªŒè¯: éœ€æ±‚ 7.1**
        """
        logger.info(f"Starting scheduler, task will run daily at {self.schedule_time}")
        
        # æ¸…é™¤ä¹‹å‰çš„è°ƒåº¦ä»»åŠ¡
        schedule.clear()
        
        # è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡
        schedule.every().day.at(self.schedule_time).do(self.run_task)
        
        self._running = True
        logger.info("Scheduler started, waiting for scheduled time...")
        
        try:
            while self._running:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            logger.info("Scheduler stopped by user")
            self._running = False
        except Exception as e:
            logger.error(f"Scheduler error: {e}", exc_info=True)
            self._running = False
            raise
    
    def stop(self):
        """
        åœæ­¢è°ƒåº¦å™¨
        Stop the scheduler
        """
        logger.info("Stopping scheduler...")
        self._running = False
        schedule.clear()
    
    def run_once(self):
        """
        æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡
        Manually execute task once
        
        ç«‹å³æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬å–-åˆ†æ-æ¨é€ä»»åŠ¡ã€‚
        Immediately executes the complete fetch-analyze-push task.
        
        **éªŒè¯: éœ€æ±‚ 7.2**
        """
        logger.info("Running task manually (once)...")
        self.run_task()
        logger.info("Manual task execution completed")
