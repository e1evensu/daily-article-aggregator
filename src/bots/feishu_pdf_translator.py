"""
PDFÁøªËØëÈ£û‰π¶ÊúçÂä°

Â§ÑÁêÜÈ£û‰π¶Ê∂àÊÅØ‰∏≠ÁöÑPDFÈìæÊé•ÔºåËá™Âä®‰∏ãËΩΩÂπ∂ÁøªËØëÔºåÁÑ∂ÂêéÂèëÂõûÈ£û‰π¶„ÄÇ
"""

import logging
import os
import re
import time
import uuid
from pathlib import Path
from typing import Optional
import urllib.request

import requests

from src.paper_translator.paper_translator.processor import PaperTranslator
from src.paper_translator.paper_translator.config import config as pdf_config

logger = logging.getLogger(__name__)


class FeishuPDFTranslationService:
    """È£û‰π¶PDFÁøªËØëÊúçÂä°"""

    # ÊîØÊåÅÁöÑPDF URLÊ®°Âºè
    PDF_URL_PATTERNS = [
        r'https?://[^\s]+\.pdf',
        r'https?://[^\s]+/paper/[^\s]+\.pdf',
        r'https?://[^\s]+/pdf/[^\s]+\.pdf',
        r'https?://arxiv\.org/pdf/[^\s]+\.pdf',
    ]

    def __init__(self, config: dict):
        self.config = config
        self.enabled = config.get('enabled', False)

        # ÂàõÂª∫ËæìÂÖ•ËæìÂá∫ÁõÆÂΩï
        self.input_dir = Path(config.get('input_dir', 'data/papers/input'))
        self.output_dir = Path(config.get('output_dir', 'data/papers/output'))
        self.input_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ÂàùÂßãÂåñÁøªËØëÂô®
        if self.enabled:
            self._init_translator()

        # È£û‰π¶ÈÖçÁΩÆ
        self.feishu_config = config.get('feishu', {})

        # ‰∫ëÊñáÊ°£ÂèëÂ∏ÉÂô®
        self._doc_publisher = None

        # ÁºìÂ≠òÁøªËØëÊñáÊú¨ÁöÑÂØπË±°ÔºàÁî®‰∫éÁΩëÈ°µÁøªËØëÔºâ
        self._text_translator = None

    def _get_text_translator(self):
        """Ëé∑ÂèñÊñáÊú¨ÁøªËØëÂô®ÔºàÁî®‰∫éÁΩëÈ°µÁøªËØëÔºâ"""
        if self._text_translator is None:
            from src.paper_translator.paper_translator.translation_engine import TranslationEngine
            # TranslationEngine ‰ΩøÁî®ÂÖ®Â±ÄÈÖçÁΩÆ
            self._text_translator = TranslationEngine()
        return self._text_translator

    def _get_doc_publisher(self):
        """Ëé∑Âèñ‰∫ëÊñáÊ°£ÂèëÂ∏ÉÂô®"""
        if self._doc_publisher is None:
            from src.aggregation.feishu_doc_publisher import FeishuDocPublisher
            self._doc_publisher = FeishuDocPublisher(self.feishu_config)
        return self._doc_publisher

    def _create_translation_doc(self, title: str, content: str, original_url: str) -> str:
        """ÂàõÂª∫ÁøªËØëÂÜÖÂÆπÁöÑ‰∫ëÊñáÊ°£"""
        try:
            publisher = self._get_doc_publisher()

            # ÊûÑÂª∫ÊñáÊ°£Âùó
            blocks = []

            # Ê†áÈ¢ò
            blocks.append(publisher._create_heading_block(f"üìÑ {title}", level=1))

            # ÂéüÊñáÈìæÊé•
            blocks.append(publisher._create_text_block(f"üîó ÂéüÊñáÈìæÊé•: {original_url}"))

            blocks.append(publisher._create_divider_block())

            # ÂÜÖÂÆπÔºàÂàÜÊÆµÊ∑ªÂä†ÔºåÊØèÂùóÊúâÈôêÂà∂Ôºâ
            max_block_size = 8000
            for i in range(0, len(content), max_block_size):
                chunk = content[i:i+max_block_size]
                blocks.append(publisher._create_text_block(chunk))

            # ÂàõÂª∫ÊñáÊ°£
            success, doc_url = publisher.create_document(
                title=f"[ÁøªËØë] {title}",
                blocks=blocks
            )

            if success and doc_url:
                logger.info(f"‰∫ëÊñáÊ°£ÂàõÂª∫ÊàêÂäü: {doc_url}")
                return doc_url
            else:
                logger.warning("‰∫ëÊñáÊ°£ÂàõÂª∫Â§±Ë¥•")
                return ""

        except Exception as e:
            logger.error(f"ÂàõÂª∫‰∫ëÊñáÊ°£Â§±Ë¥•: {e}")
            return ""

        logger.info(f"FeishuPDFTranslationService initialized: enabled={self.enabled}")

    def _init_translator(self):
        """ÂàùÂßãÂåñÁøªËØëÂô®"""
        # ÈÖçÁΩÆËÆ∫ÊñáÁøªËØëÁ≥ªÁªü
        pdf_config._config['deepseek_api_key'] = self.config.get('deepseek', {}).get('api_key', '')
        pdf_config._config['deepseek_base_url'] = self.config.get('deepseek', {}).get('base_url', 'https://api.deepseek.com')
        pdf_config._config['deepseek_model'] = self.config.get('deepseek', {}).get('model', 'deepseek-chat')
        pdf_config._config['siliconflow_api_key'] = self.config.get('siliconflow', {}).get('api_key', '')
        pdf_config._config['output_dir'] = str(self.output_dir)

        self.translator = PaperTranslator()
        logger.info("PDF Translator initialized")

    def is_pdf_url(self, text: str) -> bool:
        """Ê£ÄÊü•ÊñáÊú¨ÊòØÂê¶ÂåÖÂê´PDFÈìæÊé•"""
        for pattern in self.PDF_URL_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False

    def extract_pdf_url(self, text: str) -> Optional[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñPDFÈìæÊé•"""
        for pattern in self.PDF_URL_PATTERNS:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        return None

    def process_pdf_link(
        self,
        pdf_url: str,
        user_id: Optional[str] = None,
        chat_id: Optional[str] = None,
        feishu_client: Optional[object] = None
    ) -> dict:
        """
        Â§ÑÁêÜPDFÈìæÊé•

        Args:
            pdf_url: PDFÊñá‰ª∂URL
            user_id: Áî®Êà∑IDÔºàÁî®‰∫éÈÄöÁü•Ôºâ
            chat_id: Áæ§ËÅäID
            feishu_client: È£û‰π¶ÂÆ¢Êà∑Á´ØÔºàÁî®‰∫éÂèëÈÄÅÊ∂àÊÅØÔºâ

        Returns:
            Â§ÑÁêÜÁªìÊûúÂ≠óÂÖ∏
        """
        if not self.enabled:
            return {
                'success': False,
                'message': 'PDFÁøªËØëÊúçÂä°Êú™ÂêØÁî®'
            }

        logger.info(f"ÂºÄÂßãÂ§ÑÁêÜ: {pdf_url}")
        start_time = time.time()

        try:
            # 0. Â§ÑÁêÜËæìÂÖ•ÔºåÂà§Êñ≠ÊòØ PDF ËøòÊòØÁΩëÈ°µ
            cleaned_url = pdf_url.strip()

            # Ê£ÄÊü•ÊòØÂê¶ÊòØÁ∫Ø arXiv IDÔºàÂ¶Ç 2501.12345Ôºâ
            import re
            if re.match(r'^\d{4}\.\d{4,5}$', cleaned_url):
                # ËΩ¨Êç¢‰∏∫ arXiv PDF URL
                cleaned_url = f"https://arxiv.org/pdf/{cleaned_url}.pdf"
                logger.info(f"Â∑≤Â∞Ü arXiv ID ËΩ¨Êç¢‰∏∫ PDF URL: {cleaned_url}")
            elif cleaned_url.startswith('arxiv:'):
                arxiv_id = cleaned_url[6:].strip()
                cleaned_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                logger.info(f"Â∑≤Â∞Ü arXiv ID ËΩ¨Êç¢‰∏∫ PDF URL: {cleaned_url}")
            elif cleaned_url.startswith('http://') or cleaned_url.startswith('https://'):
                # ÊòØ HTTP URLÔºåÊ£ÄÊü•ÊòØ PDF ËøòÊòØÁΩëÈ°µ
                if '.pdf' in cleaned_url.lower() or '/pdf/' in cleaned_url.lower():
                    # PDF URL
                    pass
                else:
                    # ÁΩëÈ°µ URLÔºå‰ΩøÁî®ÁΩëÈ°µÁøªËØë
                    logger.info(f"Ê£ÄÊµã‰∏∫ÁΩëÈ°µ URLÔºå‰ΩøÁî®ÁΩëÈ°µÁøªËØë: {cleaned_url}")
                    return self._translate_webpage(cleaned_url)

            # 1. ‰∏ãËΩΩPDF
            pdf_path = self._download_pdf(cleaned_url)
            if not pdf_path:
                # ‰∏ãËΩΩÂ§±Ë¥•ÔºåÂ∞ùËØï‰Ωú‰∏∫ÁΩëÈ°µÂ§ÑÁêÜ
                logger.info("PDF ‰∏ãËΩΩÂ§±Ë¥•ÔºåÂ∞ùËØï‰Ωú‰∏∫ÁΩëÈ°µÂ§ÑÁêÜ")
                return self._translate_webpage(cleaned_url)

            logger.info(f"PDF‰∏ãËΩΩÊàêÂäü: {pdf_path}")

            # 2. ÁøªËØëPDF
            result = self.translator.translate(str(pdf_path))

            # 3. ÊûÑÂª∫ÁøªËØëÊñáÊú¨ÂÜÖÂÆπÁî®‰∫éÂàõÂª∫‰∫ëÊñáÊ°£
            translated_content = []
            for page in result.pages:
                page_text = []
                for block in page.translated_blocks:
                    if hasattr(block, 'text') and block.text:
                        page_text.append(block.text)
                    elif hasattr(block, 'content') and block.content:
                        page_text.append(block.content)
                if page_text:
                    translated_content.append(f"\n--- Á¨¨ {page.page_number} È°µ ---\n")
                    translated_content.append("\n".join(page_text))

            full_translated_text = "\n".join(translated_content)

            # 4. ÂàõÂª∫‰∫ëÊñáÊ°£
            doc_url = ""
            if full_translated_text:
                logger.info("Ê≠£Âú®ÂàõÂª∫‰∫ëÊñáÊ°£...")
                doc_url = self._create_translation_doc(
                    title=result.title or "ËÆ∫ÊñáÁøªËØë",
                    content=full_translated_text,
                    original_url=cleaned_url
                )

            # 5. ÂèëÈÄÅÁªìÊûúÂà∞È£û‰π¶
            if feishu_client:
                self._send_to_feishu(
                    feishu_client,
                    chat_id,
                    user_id,
                    result
                )

            processing_time = time.time() - start_time

            return {
                'success': True,
                'message': 'ÁøªËØëÂÆåÊàê',
                'output_path': result.output_path,
                'doc_url': doc_url,
                'processing_time': processing_time,
                'stats': {
                    'pages': result.total_pages,
                    'terms': len(result.all_terms),
                    'formulas': len(result.all_formulas),
                    'figures': len(result.all_figures)
                }
            }

        except Exception as e:
            logger.error(f"PDFÁøªËØëÂ§±Ë¥•: {e}")
            return {
                'success': False,
                'message': f'ÁøªËØëÂ§±Ë¥•: {str(e)}'
            }

    def _download_pdf(self, url: str) -> Optional[Path]:
        """‰∏ãËΩΩPDFÊñá‰ª∂"""
        try:
            # ÁîüÊàêÂîØ‰∏ÄÊñá‰ª∂Âêç
            filename = f"{uuid.uuid4().hex[:8]}.pdf"
            output_path = self.input_dir / filename

            # ‰∏ãËΩΩÊñá‰ª∂
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            request = urllib.request.Request(url, headers=headers)

            with urllib.request.urlopen(request, timeout=60) as response:
                with open(output_path, 'wb') as f:
                    f.write(response.read())

            return output_path

        except Exception as e:
            logger.error(f"‰∏ãËΩΩPDFÂ§±Ë¥•: {e}")
            return None

    def _fetch_webpage(self, url: str) -> Optional[dict]:
        """Ëé∑ÂèñÁΩëÈ°µÂÜÖÂÆπ"""
        import time

        try:
            from bs4 import BeautifulSoup

            # Â∞ùËØïÂ§öÊ¨°
            for attempt in range(3):
                try:
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                        'Connection': 'keep-alive',
                    }

                    response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)
                    response.raise_for_status()
                    response.encoding = response.apparent_encoding or 'utf-8'
                    break

                except Exception as e:
                    if attempt < 2:
                        logger.warning(f"Ëé∑ÂèñÁΩëÈ°µÂ§±Ë¥• (Â∞ùËØï {attempt+1}/3): {e}")
                        time.sleep(2)
                    else:
                        raise

            # Â¶ÇÊûúÊâÄÊúâÂ∞ùËØïÈÉΩÂ§±Ë¥•
            if not response:
                logger.error(f"Ëé∑ÂèñÁΩëÈ°µÂÜÖÂÆπÂ§±Ë¥•: {url}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # ÁßªÈô§‰∏çÈúÄË¶ÅÁöÑÂÖÉÁ¥†
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript', 'form', 'input', 'button', 'canvas']):
                tag.decompose()

            # ÁßªÈô§Â∏¶ÊúâÂπøÂëä„ÄÅËØÑËÆ∫„ÄÅÊé®ËçêÁ≠âÂô™Â£∞ class/id ÁöÑÂÖÉÁ¥†
            noise_selectors = [
                '[class*="comment"]', '[class*="review"]', '[class*="sidebar"]',
                '[class*="footer"]', '[class*="header"]', '[class*="nav"]',
                '[class*="social"]', '[class*="share"]', '[class*="related"]',
                '[class*="recommend"]', '[class*="ad-"]', '[class*="popup"]',
                '[class*="citation"]', '[class*="reference"]', '[class*="author"]',
                '[class*="bio"]', '[class*="profile"]', '[class*="metadata"]',
                '[class*="dataset"]', '[class*="model"]', '[class*="space"]',
                '[class*="collection"]', '[class*="bibtex"]', '[class*="citing"]',
                '[class*="similar"]', '[class*="more-like"]',
                '[id*="comment"]', '[id*="sidebar"]', '[id*="footer"]',
                '[id*="header"]', '[id*="nav"]', '[id*="citation"]',
                # ÁâπÂÆö‰∫é HuggingFace ÁöÑÂô™Â£∞
                '[class*="lg:w-"]', '[class*="rounded-"]',
            ]
            for selector in noise_selectors:
                for elem in soup.select(selector):
                    elem.decompose()

            # Ëé∑ÂèñÊ†áÈ¢ò
            title = soup.title.string if soup.title else ''
            if not title:
                h1 = soup.find('h1')
                title = h1.get_text(strip=True) if h1 else url

            # Ê∏ÖÁêÜÊ†áÈ¢ò
            title = title.split('|')[0].split('-')[0].split('‚Äî')[0].strip()

            # Ëé∑Âèñ‰∏ªË¶ÅÂÜÖÂÆπ - Êåâ‰ºòÂÖàÁ∫ßÊü•Êâæ
            content = None

            # 1. Êâæ article Ê†áÁ≠æ
            content = soup.find('article')

            # 2. Êâæ main Ê†áÁ≠æ
            if not content:
                content = soup.find('main')

            # 3. Êâæ class ÂåÖÂê´ content„ÄÅpost„ÄÅarticle„ÄÅentry ÁöÑ div
            if not content:
                for div in soup.find_all('div'):
                    cls = div.get('class', [])
                    if any(c in ' '.join(cls).lower() for c in ['content', 'post', 'article', 'entry', 'text', 'body']):
                        if len(div.get_text(strip=True)) > 500:
                            content = div
                            break

            # ÊèêÂèñÊñáÊú¨
            if content:
                # Âè™‰øùÁïôÊÆµËêΩÊñáÊú¨
                paragraphs = []
                for p in content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                    text = p.get_text(strip=True)
                    if len(text) > 20:  # ËøáÊª§Â§™Áü≠ÁöÑÊñáÊú¨
                        paragraphs.append(text)
                text = '\n\n'.join(paragraphs)
            else:
                # ÂÖúÂ∫ïÔºöËé∑ÂèñÊâÄÊúâÊñáÊú¨
                text = soup.get_text(separator='\n', strip=True)

            # Ê∏ÖÁêÜÊñáÊú¨ÔºöÁßªÈô§Á©∫Ë°åÂíåÂ§ö‰ΩôÁ©∫ÁôΩ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            text = '\n'.join(lines)

            # ‰∏çÈôêÂà∂ÊñáÊú¨ÈïøÂ∫¶

            logger.info(f"ÁΩëÈ°µËé∑ÂèñÊàêÂäü: {url}, Ê†áÈ¢ò: {title}, ÂÜÖÂÆπÈïøÂ∫¶: {len(text)}")
            return {
                'title': title,
                'content': text,
                'url': url
            }

        except Exception as e:
            logger.error(f"Ëé∑ÂèñÁΩëÈ°µÂ§±Ë¥•: {e}")
            return None

    def _translate_webpage(self, url: str) -> dict:
        """ÁøªËØëÁΩëÈ°µÂÜÖÂÆπ"""
        logger.info(f"ÂºÄÂßãÁøªËØëÁΩëÈ°µ: {url}")

        # Ëé∑ÂèñÁΩëÈ°µÂÜÖÂÆπ
        webpage = self._fetch_webpage(url)
        if not webpage:
            return {
                'success': False,
                'message': 'Ëé∑ÂèñÁΩëÈ°µÂÜÖÂÆπÂ§±Ë¥•'
            }

        # ÁøªËØëÊ†áÈ¢ò - ÁÆÄÂåñ
        translator = self._get_text_translator()
        try:
            title_translated = translator.translate_text(
                webpage['title'],
                style="casual"
            )
        except Exception as e:
            logger.warning(f"Ê†áÈ¢òÁøªËØëÂ§±Ë¥•: {e}")
            title_translated = webpage['title']

        # ÁøªËØëÂÜÖÂÆπÔºàÂàÜÊÆµÂ§ÑÁêÜÔºâ
        content = webpage['content']
        max_chunk = 6000
        chunks = [content[i:i+max_chunk] for i in range(0, len(content), max_chunk)]

        translated_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"ÁøªËØëÁΩëÈ°µÂÜÖÂÆπ chunk {i+1}/{len(chunks)}")
            try:
                # ÁÆÄÂåñÁøªËØëÊèêÁ§∫ÔºåÁõ¥Êé•ÁøªËØë
                translated = translator.translate_text(chunk, style="casual")
                translated_chunks.append(translated)
            except Exception as e:
                logger.warning(f"ÂÜÖÂÆπÁøªËØëÂ§±Ë¥• chunk {i+1}: {e}")
                translated_chunks.append(chunk)

        translated_content = '\n\n'.join(translated_chunks)

        # ÂàõÂª∫‰∫ëÊñáÊ°£
        doc_url = self._create_translation_doc(
            title=title_translated,
            content=translated_content,
            original_url=url
        )

        return {
            'success': True,
            'message': 'ÁΩëÈ°µÁøªËØëÂÆåÊàê',
            'title': webpage['title'],
            'title_translated': title_translated,
            'content': translated_content,
            'url': url,
            'doc_url': doc_url
        }

    def _send_to_feishu(
        self,
        feishu_client,
        chat_id: Optional[str],
        user_id: Optional[str],
        result
    ):
        """ÂèëÈÄÅÁøªËØëÁªìÊûúÂà∞È£û‰π¶"""
        try:
            # ÊûÑÂª∫Ê∂àÊÅØÂÜÖÂÆπ
            message = f"‚úÖ ËÆ∫ÊñáÁøªËØëÂÆåÊàêÔºÅ\n\n"
            message += f"üìÑ Ê†áÈ¢ò: {result.title}\n"
            message += f"üìä È°µÊï∞: {result.total_pages}\n"
            message += f"üìñ ÊúØËØ≠Êï∞: {len(result.all_terms)}\n"
            message += f"üî¢ ÂÖ¨ÂºèÊï∞: {len(result.all_formulas)}\n"
            message += f"üñºÔ∏è ÂõæË°®Êï∞: {len(result.all_figures)}\n"
            message += f"‚è±Ô∏è Â§ÑÁêÜÊó∂Èó¥: {result.processing_time:.1f}Áßí\n\n"

            if result.output_path:
                # ‰∏ä‰º†Êñá‰ª∂Âà∞È£û‰π¶
                file_url = self._upload_file_to_feishu(feishu_client, result.output_path)
                if file_url:
                    message += f"üì• ‰∏ãËΩΩÁøªËØëÂêéÁöÑPDF: {file_url}"

            # ÂèëÈÄÅÂà∞Áæ§ËÅä
            if chat_id and hasattr(feishu_client, 'send_message'):
                feishu_client.send_message(chat_id, message)
            elif user_id and hasattr(feishu_client, 'send_dm'):
                feishu_client.send_dm(user_id, message)

        except Exception as e:
            logger.error(f"ÂèëÈÄÅÈ£û‰π¶Ê∂àÊÅØÂ§±Ë¥•: {e}")

    def _upload_file_to_feishu(self, feishu_client, file_path: str) -> Optional[str]:
        """‰∏ä‰º†Êñá‰ª∂Âà∞È£û‰π¶Âπ∂ËøîÂõû‰∏ãËΩΩÈìæÊé•"""
        try:
            # È£û‰π¶Êñá‰ª∂‰∏ä‰º†API
            upload_url = "https://open.feishu.cn/open-apis/drive/v1/files/upload_all"

            with open(file_path, 'rb') as f:
                files = {'file': (Path(file_path).name, f, 'application/pdf')}
                data = {
                    'parent_node': 'root',
                    'file_type': 'pdf'
                }
                headers = {
                    'Authorization': f'Bearer {feishu_client.access_token}'
                } if hasattr(feishu_client, 'access_token') else {}

                response = requests.post(
                    upload_url,
                    files=files,
                    data=data,
                    headers=headers,
                    timeout=60
                )

            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    return result.get('data', {}).get('download_url')

        except Exception as e:
            logger.error(f"‰∏ä‰º†Êñá‰ª∂Âà∞È£û‰π¶Â§±Ë¥•: {e}")

        return None
