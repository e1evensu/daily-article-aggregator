# 标题去重性能优化补丁
# 问题：每次查询都 SELECT * FROM articles，导致17,506篇文章需要1.02亿次字符串比较
# 解决方案：缓存已有文章标题 + 批量去重 + 进度日志

--- a/src/repository.py
+++ b/src/repository.py
@@ -16,6 +16,8 @@ import sqlite3
 import time
 import functools
 import logging
+from concurrent.futures import ThreadPoolExecutor
+from typing import List, Tuple
 from datetime import datetime
 from difflib import SequenceMatcher
 from typing import Any, Callable, TypeVar
@@ -73,6 +75,9 @@ class ArticleRepository:
         self.db_path = db_path
         self.timeout = timeout
         self._connection: sqlite3.Connection | None = None
+        # 缓存已有文章标题，避免重复查询
+        self._cached_titles: List[Tuple[int, str]] | None = None
+        self._cache_dirty = True

     def _get_connection(self) -> sqlite3.Connection:
         """
@@ -103,6 +108,25 @@ class ArticleRepository:
         if self._connection is not None:
             self._connection.close()
             self._connection = None
+        self._cached_titles = None
+        self._cache_dirty = True
+
+    def _load_titles_cache(self) -> List[Tuple[int, str]]:
+        """加载所有文章标题到内存缓存（只加载一次）"""
+        if not self._cache_dirty and self._cached_titles is not None:
+            return self._cached_titles
+
+        conn = self._get_connection()
+        cursor = conn.cursor()
+        cursor.execute("SELECT id, title FROM articles")
+        rows = cursor.fetchall()
+        self._cached_titles = [(row['id'], row['title']) for row in rows]
+        self._cache_dirty = False
+        logger.info(f"标题缓存已加载: {len(self._cached_titles)} 篇文章")
+        return self._cached_titles
+
+    def _invalidate_cache(self):
+        """使缓存失效（新增文章后调用）"""
+        self._cache_dirty = True
+        self._cached_titles = None

     @retry_on_locked(max_retries=5, base_delay=0.1)
     def init_db(self):
@@ -191,19 +215,17 @@ class ArticleRepository:
         Returns:
             相似文章的字典，如果不存在返回None
         """
+        cached_titles = self._load_titles_cache()
+
+        for article_id, existing_title in cached_titles:
+            similarity = SequenceMatcher(None, title, existing_title).ratio()
+
+            if similarity >= threshold:
+                return self.get_by_id(article_id)
+
+        return None
+
+    def batch_find_similar(self, titles: List[str], threshold: float = 0.8) -> List[int]:
+        """
+        批量检查标题相似度（优化版）
+
+        Args:
+            titles: 标题列表
+            threshold: 相似度阈值
+
+        Returns:
+            相似文章索引列表（不相似的返回-1）
+        """
+        cached_titles = self._load_titles_cache()
+        results = []
+
+        for title in titles:
+            found = False
+            for article_id, existing_title in cached_titles:
+                if SequenceMatcher(None, title, existing_title).ratio() >= threshold:
+                    results.append(article_id)
+                    found = True
+                    break
+            if not found:
+                results.append(-1)
+
+        return results

     def save_article(self, article: dict[str, Any]) -> int:
         """
@@ -256,6 +278,7 @@ class ArticleRepository:
         finally:
             cursor.close()

         conn.commit()
+        self._invalidate_cache()  # 新增文章后失效缓存

         return cursor.lastrowid


--- a/src/scheduler.py
+++ b/src/scheduler.py
@@ -731,16 +731,34 @@ class Scheduler:
             logger.info(f"Total new articles to process: {len(all_articles)}")

             # 步骤3: 检查数据库去重（二次确认，主要用于标题相似度去重）
             logger.info("Step 3: Checking for duplicates (title similarity)...")
             new_articles = []
-            for article in all_articles:
+
+            # 批量去重优化：每100篇打印进度
+            batch_size = 100
+            total = len(all_articles)
+
+            for i, article in enumerate(all_articles):
                 url = article.get('url', '')
                 title = article.get('title', '')
+
+                # 进度日志
+                if i % batch_size == 0 or i == total - 1:
+                    logger.info(f"去重进度: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
+
                 # URL去重
                 if repository.exists_by_url(url):
                     logger.debug(f"Skipping duplicate URL: {url}")
                     continue
+
                 # 标题相似度去重
                 similar = repository.find_similar_by_title(title)
                 if similar:
