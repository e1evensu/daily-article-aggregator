#!/usr/bin/env python3
"""
å•ç‹¬è¯„ä¼°æ–°OPMLæ–‡ä»¶å¹¶åˆå¹¶åˆ°ç°æœ‰æŠ¥å‘Š

ç”¨æ³•:
    python scripts/evaluate_new_opml.py rss/hn-popular-blogs-2025.opml
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import load_config
from src.analyzers.ai_analyzer import AIAnalyzer
from src.evaluators.rss_evaluator import RSSEvaluator, FeedEvaluation


def load_existing_checkpoint(checkpoint_path: str) -> list[dict]:
    """åŠ è½½ç°æœ‰æ£€æŸ¥ç‚¹"""
    if Path(checkpoint_path).exists():
        with open(checkpoint_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('evaluations', [])
    return []


def save_merged_checkpoint(checkpoint_path: str, evaluations: list[dict]):
    """ä¿å­˜åˆå¹¶åçš„æ£€æŸ¥ç‚¹"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)
    checkpoint_data = {
        'timestamp': datetime.now().isoformat(),
        'count': len(evaluations),
        'evaluations': evaluations
    }
    with open(checkpoint_path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(evaluations)} ä¸ªè¯„ä¼°ç»“æœ")


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python scripts/evaluate_new_opml.py <opml_file>")
        print("ç¤ºä¾‹: python scripts/evaluate_new_opml.py rss/hn-popular-blogs-2025.opml")
        sys.exit(1)
    
    opml_path = sys.argv[1]
    if not Path(opml_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {opml_path}")
        sys.exit(1)
    
    print(f"ğŸ“‚ è¯„ä¼°OPML: {opml_path}")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    config = load_config("config.yaml")
    
    # åˆå§‹åŒ–AIåˆ†æå™¨
    ai_config = config.get('ai', {})
    ai_analyzer = AIAnalyzer(ai_config)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = RSSEvaluator(ai_analyzer, config)
    
    # æ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path = "reports/evaluation_checkpoint.json"
    report_path = "reports/evaluation_report.md"
    filtered_opml_path = "reports/filtered_feeds.opml"
    
    # åŠ è½½ç°æœ‰è¯„ä¼°ç»“æœ
    existing_evals = load_existing_checkpoint(checkpoint_path)
    existing_urls = {e['url'] for e in existing_evals}
    print(f"ğŸ“Š ç°æœ‰è¯„ä¼°ç»“æœ: {len(existing_evals)} ä¸ª")
    
    # è¯„ä¼°æ–°OPMLï¼ˆåªè¯„ä¼°ä¸åœ¨ç°æœ‰ç»“æœä¸­çš„ï¼‰
    print(f"\nğŸ” å¼€å§‹è¯„ä¼°æ–°è®¢é˜…æº...")
    new_evaluations = evaluator.evaluate_all_feeds(
        opml_path,
        checkpoint_path=None,  # ä¸ä½¿ç”¨æ–­ç‚¹ï¼Œç›´æ¥è¯„ä¼°
        concurrency=3,
        feed_timeout=60
    )
    
    # è¿‡æ»¤å‡ºçœŸæ­£æ–°çš„è¯„ä¼°ç»“æœ
    truly_new = []
    for eval_obj in new_evaluations:
        if eval_obj.url not in existing_urls:
            truly_new.append({
                'url': eval_obj.url,
                'name': eval_obj.name,
                'last_updated': eval_obj.last_updated,
                'is_active': eval_obj.is_active,
                'quality_score': eval_obj.quality_score,
                'originality_score': eval_obj.originality_score,
                'technical_depth': eval_obj.technical_depth,
                'categories': eval_obj.categories,
                'recommendation': eval_obj.recommendation,
                'sample_articles': eval_obj.sample_articles,
                'failure_reason': eval_obj.failure_reason
            })
    
    print(f"\nâœ¨ æ–°å¢è¯„ä¼°: {len(truly_new)} ä¸ª")
    
    # åˆå¹¶ç»“æœ
    merged_evals = existing_evals + truly_new
    
    # ä¿å­˜åˆå¹¶åçš„æ£€æŸ¥ç‚¹
    save_merged_checkpoint(checkpoint_path, merged_evals)
    
    # è½¬æ¢ä¸ºFeedEvaluationå¯¹è±¡ç”¨äºç”ŸæˆæŠ¥å‘Š
    all_feed_evals = []
    for e in merged_evals:
        all_feed_evals.append(FeedEvaluation(
            url=e['url'],
            name=e['name'],
            last_updated=e.get('last_updated', ''),
            is_active=e.get('is_active', False),
            quality_score=e.get('quality_score', 0.0),
            originality_score=e.get('originality_score', 0.0),
            technical_depth=e.get('technical_depth', 'medium'),
            categories=e.get('categories', []),
            recommendation=e.get('recommendation', 'review'),
            sample_articles=e.get('sample_articles', []),
            failure_reason=e.get('failure_reason', '')
        ))
    
    # ç”Ÿæˆåˆå¹¶åçš„æŠ¥å‘Š
    report = evaluator.generate_report(all_feed_evals)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“ æŠ¥å‘Šå·²æ›´æ–°: {report_path}")
    
    # å¯¼å‡ºç­›é€‰åçš„OPML
    kept_count = evaluator.export_filtered_opml(all_feed_evals, filtered_opml_path)
    print(f"ğŸ“¤ ç­›é€‰OPMLå·²æ›´æ–°: {filtered_opml_path} ({kept_count} ä¸ª)")
    
    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆå¹¶åç»Ÿè®¡:")
    print(f"   æ€»è®¢é˜…æº: {len(merged_evals)}")
    print(f"   æ¨èä¿ç•™: {sum(1 for e in merged_evals if e.get('recommendation') == 'keep')}")
    print(f"   å»ºè®®ç§»é™¤: {sum(1 for e in merged_evals if e.get('recommendation') == 'remove')}")
    print(f"   éœ€è¦å®¡æ ¸: {sum(1 for e in merged_evals if e.get('recommendation') == 'review')}")


if __name__ == "__main__":
    main()
